# -*- coding: utf-8 -*-
"""615804.ADAMS Final Assignment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Q44rgPcYlmnNA5MXRsKulZlL7KiH3eP5

# ADAMS SoSe 21 Task - Mert Erdinc 615804
## "Predicting prices of AirBnB listings in London"

### Introduction

Nowadays, Natural Language Processing (NLP) is getting more attention by different kind of sectors and industries, including commercial companies. Natural Language Processing can be used to classify emails, rate customer reviews' positivity, building language translaters, building chat bots for customer service and the like. Our task was to predict the prices of AirBnB listings in London using structured data, e.g. categorical, numerical, and unstructured data, e.g. text data, applying NLP models, which is the main focus of the assignment

### Table of Contents

1- Importing Libraries & Loading the Data
2- Data Analysis, Feature Engineering & Selection
3- Model Selection
4- Prediction
"""

# Commented out IPython magic to ensure Python compatibility.
import pickle
import matplotlib.pyplot as plt
# %matplotlib inline
import pandas as pd
import numpy as np
import seaborn as sns
import re
import datetime
import time

#!pip install textstat
from bs4 import BeautifulSoup
from collections import Counter
from nltk.tokenize.treebank import TreebankWordDetokenizer
from nltk.tokenize import word_tokenize
from nltk.tokenize import sent_tokenize
from nltk.corpus import wordnet
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import KFold
from sklearn.model_selection import train_test_split

#!pip install -U gensim
from gensim.models import KeyedVectors
from gensim.models.keyedvectors import Word2VecKeyedVectors
# Gensim
import gensim
import gensim.corpora as corpora
from gensim.utils import simple_preprocess
from gensim.models import CoherenceModel

#!pip install -U keras
#!pip install -U tensorflow

from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential, Model
from keras.layers import Input, concatenate, Dense, Embedding, LSTM, GRU, Bidirectional, BatchNormalization, Dropout
from keras.layers.embeddings import Embedding
from keras.initializers import Constant
#import tensorflow as tf
from IPython.display import Image

from scipy import stats
from statsmodels.graphics.gofplots import qqplot

import spacy
from sklearn.feature_extraction.text import CountVectorizer


from tensorflow.keras import activations, losses
from keras.callbacks import EarlyStopping
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
import xgboost as xgb
# %matplotlib inline
plt.rcParams["figure.figsize"] = (12,6)
seed = 123


from tqdm import tqdm
from sklearn.preprocessing import PowerTransformer

from sklearn.preprocessing import MinMaxScaler
minmaxscaler = MinMaxScaler()
from sklearn.model_selection import train_test_split
from datetime import timedelta
now = pd.Timestamp('now')

import xgboost as xgb
from sklearn.model_selection import RepeatedKFold, cross_val_score

from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences


# for assessment
from sklearn.metrics import mean_squared_error
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

import sklearn
from sklearn import neural_network
from sklearn.model_selection import RandomizedSearchCV

# Standard NLP workflow
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import wordnet
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize.treebank import TreebankWordDetokenizer
#nltk.download('punkt')
#nltk.download('stopwords')
#nltk.download('wordnet')
#nltk.download('averaged_perceptron_tagger')

import warnings
# settings
warnings.filterwarnings("ignore")
# %matplotlib inline

from collections import Counter

pd.set_option('display.max_columns', None)

train = pd.read_csv(r'C:\Users\Administrator\Downloads\adams2021\train.csv', index_col = "listing_id")
target = pd.read_csv(r'C:\Users\Administrator\Downloads\adams2021\test.csv', index_col = "listing_id")
reviews = pd.read_csv(r'C:\Users\Administrator\Downloads\adams2021\reviews.csv', index_col = "listing_id")

train = train.rename(columns={'neighborhood_overview': 'neighbourhood_overview'}) #name fixation
target = target.rename(columns={'neighborhood_overview': 'neighbourhood_overview'})

train.info() #17.7+ MB of memory usage before transformations, feature selection and data partitioning

train_copy = train.copy()           #we created copies of the data sets in case we want to go back to the initial version
train_copy2 = train_copy.copy()
target_copy = target.copy()
target_copy2 = target_copy.copy()
reviews_copy = reviews.copy()
reviews_copy2 = reviews_copy.copy()

pd.set_option('display.max_columns', None)
train.head()

"""**Interpretation:** At first glance, we observe some missings in the text data, the host since variable needs to be converted to time data, we should fix the percentages in the host response data, we should check the distributions of the numeric data and account for the outliers while we also do some scaling and normalization and finally we need to encode the categorical variables."""

df = train[['accommodates', 'bathrooms', 'bedrooms',
      'beds', 'guests_included', 'price', 'host_total_listings_count','reviews_per_month','review_scores_rating',
      'review_scores_accuracy', 'review_scores_cleanliness', 'review_scores_checkin',
     'review_scores_communication', 'review_scores_location', 'review_scores_value']]

print(len(train))
df.describe(include="all")

"""**Interpretation:**
- We can convince ourselves that there exist in the first 8 columns above.
- Except review scores rating (out of 100) the other review scores are out of 10 and they have means of close to 10, which make us suspect their informational value.
"""

#train = train_copy2   #these line are in case of errors below
#target = target_copy2
#reviews = reviews_copy2

# check for intersecting feature names       #We checked if the both data sets have the same columns
target_colnames = target.columns.values
#print(target_colnames)
train_colnames = train.columns.values
#print(train_colnames)
if np.intersect1d(train_colnames,target_colnames).size == 0:
  print("\nThere are no intersecting column names.")

"""- Below we made a list of the possible problems that we will face during the notebook in order to have some kind of a list that we could go back to during our analysis.

##### A problem regarding the informativeness of the text data:
- Maybe not to a great extent but since it is usually in good interest of hosts if they tried to market their listings with more positive texts than it would be perceived as or if they might slightly twist the truth about their listings, it is hard to make use of the text data.
- Another potential problem related to that is that because of hosts' incentive to over-emphasize good things about the listings and trying to market them, words such as "cool", "comfortable", "really", "very", "nice", "great", although ideally should indicate high prices, it is probably not the case.
- In fact, our intuition is that, the most useful text data will come from rather than the subjective comments such as "big room" (which is a somewhat vague expression as it doesn't quantify how big the room actually is), from the objective facts such as "there is a garden, private entrance, gym, indoor fireplace" and the like.

##### The problem of different languages:
Row 21452's summary and description data is in Spanish (Reasons to omit all of the languages except English: There are very few rows that contain them, and we think it is possible to have used them but we decided that it wouldn't be worth for the effort it would require)

We plan to omit different languages and keep words such as "en suite", which got into to the English language from French, as it is now is used in English also.

##### A problem in the summary column:
One of the summaries had the actual listing link at the end of the summary (but it doesn't work, probably not active anymore)

##### The problem of untranslated words in the amenities:
Some specific amenities are missings such as these "translation missing: en.hosting_amenity_49"
- This could potentially be found by comparing the actual listing's amenities and the ones in our data set if can be found on AirBnB. Infact, we tried that but weren't successfull in finding them.

##### A problem in the amenities and probably also other text data:
Expressions such as "Rock 'n Roll" gets translated as "Rock â€™n' Roll" and expressions such as "Children's" as "Childrenâ€tms", which likely happened in the process of harvesting the data from AirBnb's website.

##### On the Amenities column:
- In AirBnb's website, "kitchen, wifi, private bathroom, heating and washer" amenities are listed as popular amenities in London. Because without an actual more in-depth text analysis, it is hard to conclude whether these amenities increase prices because they are high in demand or they decrease it because most of the listings have these, we will elaborate on the amenities column later.

##### One important point about the assignment:
Room characteristics and neighbourhood information is highly predictive and fairly simple to incorporate into our model; however, although text data can be useful too, it is harder to extract the useful part of the information in them, if there is any. And lastly,
images would be useful but only having one image of the listing and given the fact that one image is representetive of the listing only to some extent, it is also not very straight forward to use the image links.

##### The problem of typos:
- E.g. We observed typos such as "it's" instead of "its" (there has to be typos among over 55,000 listings)

##### The problem of synonymous words:

#### An issue with the transit column:

Although in the model, the information about the transportation opportunities are captured with the lattitude and the longtitude information kind of "subconsciously", because the way the transportation information is written down, or left blank could indicate a tendency in the prices of the listings, the transit column will be inclduded in the model.

##### The problem with the similar summaries:
- When we analyzed the summary column we found out that there are some repeating structures or sentences that are used in the summaries, which might mean that they are not very useful. And summaries are repetions of all of the other columns combined, so we might end up not using this column at all.

**On duplicates:**
- There are no duplicates neither in the unpartitioned training data set nor in the target data set

# FEATURE SELECTION OF STRUCTURED DATA

## Important:
- Below, there will only be some explanatory analysis of the variables with structured data. Some of the explanatory analysis will provide strong evidence of feature importance while some won't. The final decision about which variables to include in the model will be made based on a combination of data-driven approaches and some intuition if the relationship between the price and the variable is obvious or no evidence of strong importance is found.
"""

train.info()

# Encode categories properly
cat_features = ['experiences_offered', 'host_is_superhost', 'host_has_profile_pic',
               'host_identity_verified', 'host_response_time', 'neighbourhood', 'neighbourhood_cleansed', 'zipcode',
               'property_type', 'room_type', 'bed_type', 'cancellation_policy', 'host_id']
for var in cat_features:
  train[var] = train[var].astype('category')

for var in cat_features:
  target[var] = target[var].astype('category')

train['host_response_rate'] = train['host_response_rate'].astype(str)
train['host_response_rate']=train['host_response_rate'].str.replace(r'%', r'.0').astype('float')

target['host_response_rate'] = target['host_response_rate'].astype(str)
target['host_response_rate']=target['host_response_rate'].str.replace(r'%', r'.0').astype('float')
#We change the data type to string first then after getting rid of the % we convert it to float.

# Identify list of continuous features
con_features = ['host_total_listings_count', 'accommodates', 'bathrooms', 'bedrooms',
               'beds', 'price', 'guests_included', 'host_response_rate', 'review_scores_rating',
               'review_scores_accuracy', 'review_scores_cleanliness', 'review_scores_checkin', 'review_scores_communication',
               'review_scores_location', 'review_scores_value', 'reviews_per_month','latitude','longitude']

# Encode continuous features
for var in con_features:
  train[var] = train[var].astype('float32')


# Verify the conversion was successful
train.info()

con_features_except_price = ['host_total_listings_count', 'accommodates', 'bathrooms', 'bedrooms',
               'beds', 'guests_included', 'host_response_rate', 'review_scores_rating',
               'review_scores_accuracy', 'review_scores_cleanliness', 'review_scores_checkin', 'review_scores_communication',
               'review_scores_location', 'review_scores_value', 'reviews_per_month','latitude','longitude']

for var in con_features_except_price:
  target[var] = target[var].astype('float32')

"""**Interpretation:**

- We fixed the categorical and numerical variables' data types because if they were in object data type we might not be able to use some functionalities with them and we were going to do this change for the categorical values when encoding them anyways. Also, since we know that it will take some time to train our model, by reducing the memory usage by using the 32 byte format, we plan to shorten the training time of our model. Infact, to be more precise we reduced the memory usage from 17.7+ MB to 11.5+ MB.

- We left latitude and longitude value with float64 type in case we need the precision they have.

##### Why host ID won't be used:
- It is possible that some hosts might be more prone to price their property higher, irrespective of neighbourhood or other property features. However, it is quite a hard task to get information that will be useful for predicting the price given that  half of the hosts are unique both in our non-partitioned training data set as well as in the target data set, which makes it more logical to not use the variable. So, we will drop it.
"""

x = target['host_id'].nunique()                             #comparision of the unpartitioned training and target data set
y = target['host_id'].isin(train['host_id']).sum()
print('There are {} unique x in the target set.'.format(x))
print('{} of {} listings have y that we know form the training set.'.format(y, len(target)))

train_copy = train.copy()           #in case of errors below

train_copy2 = train_copy.copy()
#train = train_copy2

"""**A comment:**
- We decided to analyze the price variable further, whether there are some entry mistakes or values that seem inplausible at this stage of notebook, because we want the insights that we gather using the price variable more accurate and realistic.

**A finding:**
- We found out that the host that have put 1321 listings (highest number of listings from a host),  which all together has a price mean of 325 £ per night, has 22 listings that are priced 10 £. Because when we compared the prices of the other listings and these 10 £ listings didn't seem plausible to us, we further analyzed these listings. These listings were in the same overall quality as the other apartments, meaning that they are big, have central location, high number of beds, rooms and the like. And also because after the 10 £ apartments the prices of the next ones started from 100 £, we concluded that these prices are mistakenly there and should be 100 £. So, we fixed that.
"""

train.price[train.host_total_listings_count == 1321.0].describe()

df = train[train.host_total_listings_count == 1321.0]
df.hist(column = 'price',
        bins   = 25)

train["price"][(train["host_total_listings_count"] == 1321.0) & (train["price"] == 10)] = 100 #the change
#train["price"][(train["host_total_listings_count"] == 1321.0) & (train["price"] == 10)]

#train_copy = train.copy()           #in case of errors below

train_copy2 = train_copy.copy()
#train = train_copy2

train.price.describe()

fig, ax = plt.subplots(figsize=(12, 6))
sns.distplot(train.price);

"""- We observe a skewed distribution with some outliers."""

train.hist(column = 'price',
        bins   = 50)

"""**Interpretation:**
- The histogram above helps us to realize how few the number of listings that are outliers compared to the rest of the data. (According to boxplot below the outlier upper whisker is 257.5 £)
"""

a = train[train.price == 10]
b = train[train.price == 100]
c = train[train.price == 200]
d = train[train.price == 300]
e = train[train.price == 400]
f = train[train.price == 500]


print(a[["accommodates","guests_included","bedrooms","bathrooms","beds","price"]].apply(lambda a : a.mean()))
print(b[["accommodates","guests_included","bedrooms","bathrooms","beds","price"]].apply(lambda b : b.mean()))
print(c[["accommodates","guests_included","bedrooms","bathrooms","beds","price"]].apply(lambda c : c.mean()))
print(d[["accommodates","guests_included","bedrooms","bathrooms","beds","price"]].apply(lambda d : d.mean()))
print(e[["accommodates","guests_included","bedrooms","bathrooms","beds","price"]].apply(lambda e : e.mean()))
print(f[["accommodates","guests_included","bedrooms","bathrooms","beds","price"]].apply(lambda f : f.mean()))

"""**Analysis of the output above:**
- Except the 500 £ apartments mean accommodates kept increasing as the prices were increasing.
- Guests_included variable had a steady increase as prices increase.
- Except the 500 £ apartments mean of the number of the bedrooms kept increasing.
- Except the 500 £ apartments mean of the number of the bathrooms kept increasing.
- Except the 500 £ apartments mean of the number of the beds kept increasing.

**Insight:**
- One possible argument about the findings above is that there is a switching point in the prices between 400 and 500 £, where the listings possibly start to become more about the quality of the stuff in the apartment, the location, the view, the experience rather than the size of the apartment.

**A remark:**
- The reason why we didn't plot the relationships above in a scatter plot or something similar is both because of the data size and our variables on the y-axis had at most 16 numbers (accommodates) the plots were not nice and not informative.
"""

B = train.boxplot(column = ['price'], vert = False)
B

IQR = train.price.quantile(0.75) - train.price.quantile(0.25)
factor=1.5
upper = train.price.quantile(0.75) + factor*IQR
upper

lower = train.price.quantile(0.25) - factor*IQR #but this is irrelevant since we don't have negative prices, aka debts here.
lower

x = len(train[train.price > upper])
y = len(train)

print('There are {} listings that have a price higher than 257.5.'.format(x))
print('And the listings that have a price higher than 257.5 constitutes {} percent of the data.'.format(x/y))

train.price[train.price > upper] = upper #we think that it shouldn't be a big problem that we do
                                                                   #this operation before the partitioning of the data

"""**Argumentation for truncating the target variable:**


- In the target data set it is possible that we have some outliers as we do in the unpartitioned training data set with a ratio of 3100 out of 55284. In applications of machine learning algorithms for problems that are similar to ours we assume that the training and the target data set usually shares a somewhat similar distribution of values in their variables. If we keep the outliers in the target variable we might be better at predicting the 5% of the target data but, at the same time, we would also risk predicting the 95% of the data slightly higher, which would result in an unnecessary decrease of predictive accuracy.

- Also, it probably is a good idea to truncate our target variable price because algorithms that we are planning to use uses gradient descent or back propagation, which updates the loss or the cost function in every iteration in a sense and if we have some small number of but extreme outliers, then these outliers might become over-weighted by our model and result in a bias, which would cause us to predict the prices in a less accurate way.

**The Result:** To prevent both of these possibilities above from happening, we decided to truncate our target variable price.

We will also truncate the outliers of the following variables below:
"""

fig, ax = plt.subplots(figsize=(16, 6))

train.boxplot(column = ['bathrooms','bedrooms','beds','accommodates', 'guests_included',
                         'host_total_listings_count']) #in order to observe the outliers better

IQR = train.bathrooms.quantile(0.75) - train.bathrooms.quantile(0.25)
factor=1.5
upper = train.bathrooms.quantile(0.75) + factor*IQR
upper

x = len(train[train.bathrooms > 2.25])
y = len(train)

print('There are {} listings that have a price higher than 2.25'.format(x))
print('And the listings that have a price higher than 2.25 constitutes {} percent of the data.'.format(x/y))

"""**A remark:**
- Because these numbers above and the ones we will see below seemed plausible to us, we will truncate the variable according to original upper and lower whiskers of the boxplots.
"""

train.bathrooms[train.bathrooms > upper] = upper

lower = train.bathrooms.quantile(0.25) - factor*IQR
lower

x = len(train[train.bathrooms < lower])
y = len(train)

print('There are {} listings that have the relevant variable lower than the lower value'.format(x))
print('And the listings that have the relevant variable lower than the lower value constitutes {} percent of the data.'.format(x/y))

train.bathrooms[train.bathrooms < lower] = lower

IQR = train.bedrooms.quantile(0.75) - train.bedrooms.quantile(0.25)
factor=1.5
upper = train.bedrooms.quantile(0.75) + factor*IQR
upper

x = len(train[train.bedrooms > 3.5])
y = len(train)

print('There are {} listings that have the relevant variable higher than the upper value'.format(x))
print('And the listings that have the relevant variable higher than the upper value constitutes {} percent of the data.'.format(x/y))

train.bedrooms[train.bedrooms > upper] = upper

IQR = train.beds.quantile(0.75) - train.beds.quantile(0.25)
factor=1.5
upper = train.beds.quantile(0.75) + factor*IQR
upper

x = len(train[train.beds > 3.5])
y = len(train)

print('There are {} listings that have the relevant variable higher than the upper value'.format(x))
print('And the listings that have the relevant variable higher than the upper value constitutes {} percent of the data.'.format(x/y))

train.beds[train.beds > upper] = upper

IQR = train.accommodates.quantile(0.75) - train.accommodates.quantile(0.25)
factor=1.5
upper = train.accommodates.quantile(0.75) + factor*IQR
upper

x = len(train[train.accommodates > upper])
y = len(train)

print('There are {} listings that have the relevant variable higher than the upper value'.format(x))
print('And the listings that have the relevant variable higher than the upper value constitutes {} percent of the data.'.format(x/y))

train.accommodates[train.accommodates > upper] = upper

IQR = train.guests_included.quantile(0.75) - train.guests_included.quantile(0.25)
factor=1.5
upper = train.guests_included.quantile(0.75) + factor*IQR
upper

x = len(train[train.guests_included > upper])
y = len(train)

print('There are {} listings that have a price higher than the upper value'.format(x))
print('And the listings that have a price higher than the upper value constitutes {} percent of the data.'.format(x/y))

train.guests_included[train.guests_included > upper] = upper

IQR = train.host_total_listings_count.quantile(0.75) - train.host_total_listings_count.quantile(0.25)
factor=1.5
upper = train.host_total_listings_count.quantile(0.75) + factor*IQR
upper

x = len(train[train.host_total_listings_count > upper])
y = len(train)

print('There are {} listings that have a price higher than the upper value'.format(x))
print('And the listings that have a price higher than the upper value constitutes {} percent of the data.'.format(x/y))

train.host_total_listings_count[train.host_total_listings_count > upper] = upper

"""**A remark:**
- We know that truncation generally results in a decrease in correlation and because the variables we didn't truncate above had already small amount of correlation with the price, we didn't truncate them and won't include them in the model.

##### Now we apply the same truncation process to the target data set.
"""

fig, ax = plt.subplots(figsize=(16, 6))

target.boxplot(column = ['bathrooms','bedrooms','beds','accommodates', 'guests_included']) #in order to observe the outliers better

IQR = target.bathrooms.quantile(0.75) - target.bathrooms.quantile(0.25)
factor=1.5
upper = target.bathrooms.quantile(0.75) + factor*IQR
upper

x = len(target[target.bathrooms > upper])
y = len(target)

print('There are {} listings that have the relevant variable higher than the upper value'.format(x))
print('And the listings that have the relevant variable higher than the upper value constitutes {} percent of the data.'.format(x/y))

target.bathrooms[target.bathrooms > upper] = upper

lower = target.bathrooms.quantile(0.25) - factor*IQR
lower

x = len(target[target.bathrooms < lower])
y = len(target)

print('There are {} listings that have the relevant variable lower than the lower value'.format(x))
print('And the listings that have the relevant variable lower than the lower value constitutes {} percent of the data.'.format(x/y))

target.bathrooms[target.bathrooms < lower] = lower

IQR = target.bedrooms.quantile(0.75) - target.bedrooms.quantile(0.25)
factor=1.5
upper = target.bedrooms.quantile(0.75) + factor*IQR
upper

x = len(target[target.bedrooms > upper])
y = len(target)

print('There are {} listings that have the relevant variable higher than the upper value'.format(x))
print('And the listings that have the relevant variable higher than the upper value constitutes {} percent of the data.'.format(x/y))

target.bedrooms[target.bedrooms > upper] = upper

IQR = target.beds.quantile(0.75) - target.beds.quantile(0.25)
factor=1.5
upper = target.beds.quantile(0.75) + factor*IQR
upper

x = len(target[target.beds > upper])
y = len(target)

print('There are {} listings that have the relevant variable higher than the upper value'.format(x))
print('And the listings that have the relevant variable higher than the upper value constitutes {} percent of the data.'.format(x/y))

target.beds[target.beds > upper] = upper

IQR = target.accommodates.quantile(0.75) - target.accommodates.quantile(0.25)
factor=1.5
upper = target.accommodates.quantile(0.75) + factor*IQR
upper

x = len(target[target.accommodates > upper])
y = len(target)

print('There are {} listings that have the relevant variable higher than the upper value'.format(x))
print('And the listings that have the relevant variable higher than the upper value constitutes {} percent of the data.'.format(x/y))

target.accommodates[target.accommodates > upper] = upper

IQR = target.guests_included.quantile(0.75) - target.guests_included.quantile(0.25)
factor=1.5
upper = target.guests_included.quantile(0.75) + factor*IQR
upper

x = len(target[target.guests_included > upper])
y = len(target)

print('There are {} listings that have the relevant variable higher than the upper value'.format(x))
print('And the listings that have the relevant variable higher than the upper value constitutes {} percent of the data.'.format(x/y))

target.guests_included[target.guests_included > upper] = upper

IQR = target.host_total_listings_count.quantile(0.75) - target.host_total_listings_count.quantile(0.25)
factor=1.5
upper = target.host_total_listings_count.quantile(0.75) + factor*IQR
upper

x = len(target[target.host_total_listings_count > upper])
y = len(target)

print('There are {} listings that have the relevant variable higher than the upper value'.format(x))
print('And the listings that have the relevant variable higher than the upper value constitutes {} percent of the data.'.format(x/y))

target.host_total_listings_count[target.host_total_listings_count > upper] = upper

"""**The End of the Outlier Handling**

# HOST SINCE
"""

#train_copy = train.copy()           #in case of errors below #stores the previous version of the data set (t-1)

#train_copy2 = train_copy.copy()
#train = train_copy             #stores one more previous version of the data set (t-2)

from datetime import timedelta
now = pd.Timestamp('now')

"""By using the "now" time stamp from "timedelta" package and some analysis we found that the data extraction date is 7 January, 2020. For the new "host_days" variable that we will create below to look nicer we used the data extraction date."""

train.data_extraction_date = "'2020-01-08'"
train['data_extraction_date'] = pd.to_datetime("'2020-01-08'".replace("'",""))
train.data_extraction_date = pd.to_datetime(train['data_extraction_date'], format='%Y-%m-%d')

train['host_since'] = pd.to_datetime(train['host_since'], format='%Y-%m-%d')
train['host_since'] = train['host_since'].where(train['host_since'] < train.data_extraction_date, train['host_since'] - np.timedelta64(1, 'D'))
train['host_days'] = (train.data_extraction_date - train['host_since']).astype('timedelta64[D]')

train.drop(['data_extraction_date'], axis=1, inplace = True)
train.drop(['host_since'], axis=1, inplace = True)

train['host_days'] = train['host_days'].astype('float32')

target.data_extraction_date = "'2020-01-08'" #Now the same operation for the target data set
target['data_extraction_date'] = pd.to_datetime("'2020-01-08'".replace("'",""))
target.data_extraction_date = pd.to_datetime(target['data_extraction_date'], format='%Y-%m-%d')

target['host_since'] = pd.to_datetime(target['host_since'], format='%Y-%m-%d')
target['host_since'] = target['host_since'].where(target['host_since'] < target.data_extraction_date, target['host_since'] - np.timedelta64(1, 'D'))
target['host_days'] = (target.data_extraction_date - target['host_since']).astype('timedelta64[D]')

target.drop(['data_extraction_date'], axis=1, inplace = True)
target.drop(['host_since'], axis=1, inplace = True)

target['host_days'] = target['host_days'].astype('float32')

train.host_days.describe() #just as a back check to see that the minimum day value is 1

"""- The mean duration for the hosts is 5.66 years"""

sns.distplot(train.host_days);

"""# HOST ID"""

x=train.host_id.value_counts()
x.head(20)

"""- We have checked the IDs above to see whether they have post listings that are located in different addresses, and they were indeed different, which indicates us the presence of real estate agencies operating or rich people renting some of their properties on Airbnb.

- We can also confirm the same information using the "host_total_listings_count" variable.
"""

train.host_total_listings_count.value_counts() #correlation with the price is 0.28, so important

sns.distplot(train.host_total_listings_count);

"""- The 0.28 correlation points out that we should likely use this variable in our model."""

x=train[train['host_total_listings_count'] == 1321] #we check whether the most extreme outliers are scams
print(len(x))
print(len(np.unique(x.picture_url)))
print(x.price.mean())
print(train.price.mean())

"""**Interpretation:** We found in the "host_total_listings_count" varible that two hosts have put listings a lot. We checked whether there is evidence that they are scams. There are different photos in their listings, also different addresses. But these might also be the case with a real estate agency or another business using AirBnB. One evidence that they might be scams was that they charge 3 times more than the average price in the data set and their account is not verified but they could be high-end real estate agents and there is not enough evidence to claim that some listings are scams."""

train.host_total_listings_count.describe()

"""# HOST RESPONSE TIME

###### Why data related to host response time won't be used:

The "host_response_time" variable has four categories and the "within an hour" category corresponds to 43% of all of the rows in the column while the 17802 missing rows it has corresponds to 32% of the data. So, this leaves very little informational value about the variable, thus it won't be used.

Similarly, 47% of the "host_response_rate" variable corresponds to a value of "100%" and the same 17802 missings, as the "host_response_time" variable, constitute the 32% of the "host_response_rate" variable. Thus, likewise, this variable is also out of the analysis.
"""

train.groupby(['host_response_time'])["price"].apply(lambda train : train.mean())

"""Also, in the table above, we observe no clear pattern between price means in the respective categories of the "host response time" variable."""

x=train.host_response_rate.value_counts()
print(train.host_response_rate.describe())
print(x)

"""- We could discretize this variable into 3 groups as "missings, one hundreds and the others" but because of high number of missings and most of them being 100% in host_response_rate variable, we concluded that it wouldn't make much sense."""

train.host_response_rate.describe()  #we also observe a very skewed column.

"""# HOST IS SUPERHOST"""

train.host_is_superhost.value_counts()

train.groupby(['host_is_superhost'])["price"].apply(lambda train : train.mean())

"""Probably it is a better idea to calculate the fisher score for the variable, or use other methods such as wrapper funcionts or calculating the importance of the variable by explainable AI methods but for the time being this variable and likely the upcoming ones are to be excluded.

# HOST HAS PROFILE PIC
"""

train.host_has_profile_pic.value_counts()

target.host_has_profile_pic.value_counts()

train.groupby(['host_has_profile_pic'])["price"].apply(lambda train : train.mean())

"""- Although this variable might have helped us predicting a very small portion of the listing prices more accurately, because of its extreme skewness, we won't be using this variable.

# HOST IDENTITY VERIFIED
"""

train.host_identity_verified.value_counts()  #I left the checking of the notbeook here #za

train.groupby(['host_identity_verified'])["price"].apply(lambda train : train.mean())

"""This variable isn't as imbalanced as the other ones however the price mean difference is not significant between the categories of the variable.

# GEOSPATIAL / LOCATION DATA
"""

train.neighbourhood.value_counts()

"""- Since the neighbourhood cleansed variable has 33 categories, it might be a better idea to use that rather than the original neighbourhood column."""

fig, ax = plt.subplots(figsize=(9, 9))
sns.scatterplot(train.longitude, train.latitude)
#f,ax = plt.subplots(figsize=(18, 15))

"""**Interpretation:**
- We can get a feel for our coordinate data and compare it with the map that will follow below.
"""

train.longitude.describe()

train.latitude.describe()

plot_order = train.groupby('neighbourhood_cleansed')['price'].mean().sort_values(ascending=False).index.values
g=sns.catplot(x="price", y="neighbourhood_cleansed", kind="box", orient = "h", order=plot_order, data=train)
print(g.fig.set_size_inches(7.5,7.5))

from IPython.display import Image
from IPython.core.display import HTML
Image(url= "http://www.easthanders.com/web/easthanders/images/stories/LondonBoroughMap.jpg", width=450, height=450)

"""**Interpretation:**
- Before we truncated the price variable less popular, touristy and central neighbourhoods had fewer outliers and ofcourse they still have lower price mean.
- The price range of the listings that were put by the hosts in the neighbourhoods that are high in demand like the first few ones, are on average wider than the ones put by the hosts in the less desired neighbourhoods, which makes sense in our opinion.
"""

train.neighbourhood_cleansed.value_counts() #no missings in this variable

"""### On which location data to use:

- In our opinion, in principle, if a XGB boost algorithm is trained using the coordinate data long enough, it would be able to capture all of the information that the neighbourhood or zip code data is providing. However, we are not sure if that will be the case in our case. So, we will also include neighbourhood data by WOE coding, if the correlation between neighbourhood variable and the coordinate data ends up not being so high (to prevent multicollinearity).
"""

train.zipcode.value_counts()

"""- Zip code data seems messy and needs some adjustments; however, since we believe using the coordinate and the neighbourhood data will let us capture the price and location dependency to an extent that we will be satisfied with, we decided not to use this variable.

# MOST IMPORTANT GROUP OF DATA (PROPERTY FEATURES)
"""

# These variables showed a correlation above 0.63. So we examine their relationship more deeply as follows:
sns.jointplot(x='accommodates',y='price',data=train, kind='kde');

print(train.accommodates.describe())
print(train.price.describe())

accommodates                   0.589331
bedrooms                       0.512769 #old old correlations
beds                           0.470839
bathrooms                      0.361858
guests_included                0.348483
host_total_listings_count      0.274317


accommodates                   0.631237 #old correlations
bedrooms                       0.531630
beds                           0.494946
guests_included                0.363381
bathrooms                      0.341959
host_total_listings_count      0.282928
review_scores_location         0.109788
review_scores_cleanliness      0.036373
host_response_rate             0.017617
host_days                     -0.010650
review_scores_rating          -0.012025
latitude                      -0.025406
review_scores_accuracy        -0.032054
review_scores_checkin         -0.056765
review_scores_communication   -0.062404
reviews_per_month             -0.076073
review_scores_value           -0.096005
longitude                     -0.123446

correlation = (train[train.columns].corr()['price'][:].sort_values(ascending=False)).drop('price')
print(correlation)

"""**Interpretation:**
- Below there is a full sized correlation table to also observe the other relationship between the non-target variables, but based on the correlation between only our target variable price and other variables, it seemed to us that there was a "natural" limit of correlation in the sense that after 0.28 of correlation next biggest is 0.12. Numerical reviews have to many missings and they are two skewed although they correlated 12%. So, we won't use them. And we will keep the coordinate data although they don't correlate with the price very highly.

- Longitude being different than latitude in correlation tells us that prices change more on the direction of east to west than north to south. In fact, as we learned from an internet search West London is perceived as more Posh on average.
"""

train_host.corr()

"""# PROPERTY TYPE"""

train.property_type.value_counts()

#train.loc[:, "property_type"] = train.property_type.replace("Farm stay", "Barn")
train.loc[:, "property_type"] = train.property_type.replace("Windmill", "Apartment")
train.loc[:, "property_type"] = train.property_type.replace("Plane", "Apartment")
train=train[train.property_type != ('Parking Space')]

"""- Initially, we thought Farm Stay and Barn were referring to the same property type. However, Barn's property aim is mainly for events. So, we decided to keep them as they are.

- Also, although the property type "Island" is not meant the word's original meaning since we are not applying NLP to these categories, we will just assume that it is somewhat different than the other property types. And we will WOE encode it as we will the other ones, thus we won't change it.

- Also, we found out that windmill and plane is just an apartment so we fixed it.

- And we backchecked that parking space is really a parking space, not an accommodation, and it was also not in the target data set so it is also omitted.
"""

train.groupby(['property_type'])["price"].apply(lambda train : train.mean())

"""**Interpretation:** Regarding the price means above, it is possible to gather some expected insights from them such as Villa being expensive or other types such as bus, dome house, hut is cheaper; however, these information cannot be thought seperately from location and "within property" data.

# ACCOMMODATES

From commonsense, we knew that accommodates would play an important role in our model because it gives us a clue about the size of the room or the house (square meters). Infact, we thought of creating a new variable called the "size of the place" by coming up with a formula using the other related variables such as bed number, type, whether it is a shared place or not, room type, bathrooms etc. and assigning them coefficients. However, we suspected the predictive power of the variable and the reliability. We concluded that even if we didn't create such a variable, after training our model, it would probably know the size of a place better than us by combining all of the information. And anyways, the goal is to predict the price and not the size of the place.
"""

train.accommodates.value_counts()

train.room_type.value_counts()

print(train.groupby(['room_type'])["accommodates","bathrooms","beds","price"].apply(lambda train : train.mean()))

"""**Interpretation:** One of our initial concerns was that we mistakenly predict the shared room prices higher because they have a higher number of beds on average; however, we concluded that the model would pick up the input-to-output relationship between shared room categories, number of beds and the prices. So, we decided not to follow our initial idea of predicting the following variable and converting it back to normal price after the prediction:
- Price / (bed-per-person) * (room-per-person) * (bath-per-person).
"""

train.bathrooms.value_counts()

df = train[['bathrooms','bedrooms', 'beds']]
df.hist(bins=35,figsize=(12,8));

"""##### A comment:

- In order to help the model perform better, require less training and get some burden off it, we thought that instead of using the variables "bathrooms, bedrooms, beds" as they are, using new variables where we divide those by the number of accommodates and call them "bathrooms, bedrooms, beds + per person" could be more informative. Although this idea seemed like a promising idea at first, we realized that a small bedroom might have couple of beds and a big one only one. There are also different type of beds that we needed to consider. And lastly, although bathroom per person could still be created we didn't want to make any assumptions about peoples' prefered way of living and habits and not introduce over complication to the model. Thus, we decided not to create such variables and let the model combine the interdependencies and make sense out of them by itself.
"""

df = train[['bathrooms','bedrooms', 'beds']]
df.describe()

"""**Interpretation:** Given that in every house there is at least one bathroom, bedroom and bed, it is not suprising that the means are in between 1 and 2. However, we might need to take care of the outliers, that make an exception in some of the listings."""

train.bedrooms.value_counts()

train.beds.value_counts()

train.bed_type.value_counts()

train.groupby(['bed_type'])["price"].apply(lambda train : train.mean())

"""- We observe an insight that was expected. This variable could be useful for the few listings that doesn't offer real beds."""

train.guests_included.value_counts()

"""**Interpretation:**
- This finding shows while most of the hosts don't want many guests in their places a small percentage of them are more relaxed about it.

- Also, there is 0.365871 correlation between the price. So, it is a variable that is likely to be kept.
"""

# We create one histogram for each review related data
train_reviews.hist(bins=35,figsize=(12,12));

x=train.review_scores_rating.value_counts()
x.head()

train.review_scores_accuracy.value_counts()

"""##### Comment on the numerical review data:
- Ideally the numerical review data that is out of 10 (which is now out of 5 on AirBnb) would be informative. But 24% of the rows in them are missing, the non-missing values are quite skewed towards a full score and they have low correlation with the price variable, all of which are signs that the variable is uninformative. Because a big portion of the accommodates didn't even gave scores, numerical reviews are too easy to submit and, in our opinion, it is obvious that there is a general tendency of "being nice" to the host and giving good scores to him or her for those who cared to give a review score. So, we won't use those variables.

##### Things we will assume about the textual reviews:
- If there are no reviews we assume that the stay were just as expected, nothing good or bad, and from it not much was expected or there is also always the possibility of the accommodates not bothering to leave a review.
- Long reviews are more informative than the short ones.
- We also see the negative reviews as important because they are not as common as the positive ones and in our experience, most of the time, they are informative.
"""

train.corr()

"""###### Additional findings from the correlation table above:
- Price and accommodates, bathrooms, bedrooms, beds, guests_included correlates highly, which is plausible.
- Also, host_total_listings_count and price correlates mildly, which suggests that the hosts that host more tend to have higher prices. They also tend to have bigger capacity houses as we can see from the around 10 percent correlation levels between host_total_listings_count and accommodates and bedrooms.
- The difference in the price change is significantly different between latitude and longitude variable. More specifically, the eastern that a person searches an Airbnb in London, prices tend to slightly decrease.

**Experiences Offered Interpretation:** Although 98% of the experiences offered are "None". The rest 2% can be possibly informative but our intuition is that the informativeness in this variable is captured by the numerical room and property characteristics and interaction between them. We will still try a more data-driven approach to give the final decision of keeping the variable or not. (Same applies to the room_type variable that we analyze below)
"""

train.experiences_offered.value_counts()

train.groupby(['experiences_offered'])["price"].apply(lambda train : train.mean())

"""- Below one can also visually observe the differences in means as well as the outliers."""

g=sns.catplot(x="price", y="experiences_offered", kind="box", orient = "h", data=train)
g.fig.set_size_inches(7.5,7.5)

"""**Room Type Interpretation:**
- Private and shared rooms seem to be much cheaper than the other room types.

- The table above seems to suggest that, on average, private rooms have received highest reviews (except in location), then Entire home & apartments, then shared rooms and lastly hotel rooms

- So, it seems like when there is a place that is reserved only for the accomodates, they enjoy that but we also think although there is no explicit price/performance review score, this preference is reflected in all of their review scores. So, because entire apartments are priced higher, their scores could be slightly lower. And hotel rooms and shared rooms are kind of accommodation types where the resident shares stuff with others, which are both low and close to each other in scores; however, again based on our prior argument about the price sensitivity of the renters, hotel rooms received lower ratings in general as they are more expensive.

- Lastly, when the staying experience includes sharing in a flat or apartment where people live normally as in the case of private room and shared room, people write more reviews (We guess because then the experience becomes somewhat more personal).
"""

train.room_type.value_counts()

sns.countplot(x=train.room_type, data=train) #an easier way to grasp the imbalance

train.groupby(['room_type'])["price","review_scores_rating","review_scores_accuracy","review_scores_cleanliness",
                             "review_scores_checkin","review_scores_communication","review_scores_location",
                             "review_scores_value","reviews_per_month"].apply(lambda train : train.mean())

"""**Cancellation Policy Interpretation:** By checking each categorical value in the cancellation policy variable and their corresponding price means, we see that cancellation policy can matter for our model but below we will analyze it in more detail."""

target.cancellation_policy.value_counts()

train.cancellation_policy.value_counts()

train.groupby(['cancellation_policy'])["price"].apply(lambda train : train.mean())

"""**Reasons to not include reviews_per_month variable in the model:**

- Although we would expect the variable to positively correlate with the price it correlates -0.074304, which is because negative and small is not desirable.

- Also, because the 22% of the variable is missing and the reasons above, we decided not to use this variable in our model.
"""

sns.distplot(train.reviews_per_month);

train.reviews_per_month.value_counts()

print(train.reviews_per_month.isnull().sum().sum())

"""## Feature Selection & Arguments

- Below, to give a final decision of which features to keep, we dummy encode the categorical variables, check the frequencies of the categories, check the price mean and check how each category correlate with the price. When giving our final decision we provide arguments that we formed with the help of above mentioned findings as well as our intuition and business background.
"""

#train.info()

train_copy = train.copy()           #in case of errors below

train_copy2 = train_copy.copy()
#train = train_copy2

# train = train_copy

superhost = pd.get_dummies(train.host_is_superhost)
profile_pic = pd.get_dummies(train.host_has_profile_pic)
identity = pd.get_dummies(train.host_identity_verified)
room = pd.get_dummies(train_copy2.room_type)
bed = pd.get_dummies(train.bed_type)
cancellation = pd.get_dummies(train.cancellation_policy)
property = pd.get_dummies(train.property_type)
neighbourhood = pd.get_dummies(train.neighbourhood_cleansed)


df = pd.concat([train_copy2, room], axis=1)      #we iteratively concated different dummies each time
df.corr()

"""### Final decisions whether to include certain variables and reasons for the decisions:

**Bed types:**

- As we did with the other variables we dummy encoded the categories of the variable bed type and the biggest correlation with the price was a magnitude of 0.04 while the variable being very skewed. Thus, we excluded the variable from the model.
"""

train.bed_type.value_counts()

"""**Room types:**

**1-** The price means of "Entire home/apt" and "Hotel room" were 144 £ both. But while the "Entire home/apt" had a correlation of 0.530712 with the price the "Hotel room" only had 0.047523.

**2-** The price means of "Private room" and "Shared room" were respectively 53 and 49 £, which is quite close to each other. But while the "Private room" had a correlation of -0.532197 with the price the "Shared room" only had -0.058015.

**Result:** Basically because "Entire home/apt" and "Hotel room" constituted 55, 43 percent of the rows, they were found to have high correlation. So, because of this reason, we exclude the variable.
"""

train.groupby(['room_type'])["accommodates","bathrooms","beds","price"].apply(lambda train : train.mean())

train.room_type.value_counts() #entire 60 corr, private -60 corr, hotel 0.5 corr, shared -0.6 corr with the price

"""**Cancellation policy: we will use**

- The "flexible" and "strict_14" category in the cancellation policy variable correlated with the price -0.21 and 0.20 respectively, while "super_strict_30" and "super_strict_60" 0.08 and 0.09 respectively. So, because we thought there could be some potential of being informative in this variable, we decided to keep it.

- Also, although the price mean of the listings are around 100 £, there were 2 categories in total which were only in a few listings and had a price quite different than the mean. These categories are "luxury_moderate", "luxury_super_strict_95". Both of them were only in 1 listing and both had means of 450 £.

(to be deleted) So, we exclude them. The category strict was also observed only in two listings in the unpartitioned training set but since there are also 2 listings in the target set with the cancellation policy of "strict", we decided to keep them.
"""

train.cancellation_policy.describe()

train.cancellation_policy.value_counts()

target.cancellation_policy.value_counts()

train.groupby(['cancellation_policy'])["accommodates","bathrooms","beds","price"].apply(lambda train : train.mean())

"""Superhost true corr with price: -0.04
Superhost false corr with price: 0.04

Host has profile pic true corr with price: 0.001
Host has profile pic false corr with price: -0.01

Host identity verified true corr with price: -0.02
Host identity verified false corr with price: 0.02

- There could possibly be some interaction effects but even if there is we suspect it is a strong one and if we included these variables we also had the danger of harming the model rather than improving it, so we decided these variables not to include in the model.


"""

train.host_is_superhost.value_counts()

train.host_has_profile_pic.value_counts()

train.host_identity_verified.value_counts()

"""**Property type:**

- We also won't be including the property type variable in the model because there were too many categories to handle and 67 and 21 percent of them were apartments and house respectively. Also, there were low correlation observed above, which showed us they might have some small effect on the price but they are not one of the main drivers and since we want to keep our model somewhat simpler (mostly including only the main driver variables), we chose not to include the property type variable too.
"""

print(train.property_type.isnull().sum().sum())
train.property_type.value_counts()

property_type_encoded = pd.get_dummies(train.property_type)
price_property_type_encoded = pd.concat([train.price, property_type_encoded], axis=1)
price_property_type_encoded.corr()["price"].sort_values(ascending=False)

"""**Neighbourhood cleansed:**

- It was more or less obvious the location/neighbourhood data would be one of the main determinants of the price. We confirm that below and decide to keep this variable to be used in the model.
"""

neighbourhood_cl = pd.get_dummies(train.neighbourhood_cleansed)
price_neigh_encoded = pd.concat([train.price, neighbourhood_cl], axis=1)
price_neigh_encoded.corr()["price"].sort_values(ascending=False)

"""**Experiences offered:**

- The reason why we won't be using this variable is that it correlates low with the price and is highly skewed.
"""

experiences = pd.get_dummies(train.experiences_offered)
price_property_type_encoded = pd.concat([train.price, experiences], axis=1)
price_property_type_encoded.corr()["price"].sort_values(ascending=False)

train.experiences_offered.value_counts()

train.info()

"""## Missing Values"""

train_copy = train.copy()           #in case of errors below

train_copy2 = train_copy.copy()
#train = train_copy2

#train = train_copy

fig, ax = plt.subplots(figsize=(12, 6))
sns.heatmap(train.isnull(), cbar=False)  # quick visualization of the missing values in our data set

print(train.isnull().sum().sort_values(ascending = False))

fig, ax = plt.subplots(figsize=(12, 6))
sns.heatmap(target.isnull(), cbar=False)

print(target.isnull().sum().sort_values(ascending = False))

"""**On the missing values:**

- Among the structured data we decided to use the variables beds, bedrooms and bathrooms have some missing values in both the unpartitioned training data set and the target data set. Because these variables with the missing values have high predictive power, we didn't want to teach our model some possibly wrong relationships by using the mean-imputed rows instead of the missing values. And since these only account for 0.007 of our data (394 out of 55284), we chose to exclude these observations.

- However, since we have to get a prediction for every row in the target data set, we decided to impute the missings in the  beds, bedrooms and bathrooms variables with the mode of the aggregated data set.

- Lastly, we decided to use the mode of the data because every listing has at least 1 bathroom, bed and bedroom, and probably this is the most common case in the whole data set but if we imputed the mean, then because of the outliers with more than 10 bedrooms, bathrooms, and beds, we would get a slightly higher value, which would introduce unwanted bias to our model.
"""

train_copy = train.copy()           #in case of errors below

train_copy2 = train_copy.copy()
#train = train_copy2

# train = train_copy

train_filtered_host_total_listings_count = train['host_total_listings_count'].dropna() #imputation with the mean is being done
train.host_total_listings_count = train['host_total_listings_count'].fillna(round(train_filtered_host_total_listings_count.mean()))

train = train[train['beds'].notna()]
train = train[train['bathrooms'].notna()]
train = train[train['bedrooms'].notna()]

data_agg = (pd.concat([train,target], axis=0)) #short for "data_aggregated"

target['beds'] = target['beds'].fillna(data_agg['beds'].mode()[0])
target['bathrooms'] = target['bathrooms'].fillna(data_agg['bathrooms'].mode()[0])
target['bedrooms'] = target['bedrooms'].fillna(data_agg['bedrooms'].mode()[0])

train_copy = train.copy()           #in case of errors below

train_copy2 = train_copy.copy()
#train = train_copy2

#train = train_copy

"""## Data Partioning - WOE Coding - Scaling - Normalization

#### On Normalization:  
From what we have researched on the internet, when using tree-based algorithms it doesn't make a big difference when normalization is not used and XGB is kind of an ensemble algorithm consisting of trees using gradient boosting. But as we discussed in the ADAMS Q&A session, normalization might fasten the model by taking some of the burden off the model and it is somewhat an important concept because the statistical assumptions usually have a normality assumption for them to be accurate.

**On Weight-of-Evidence Coding:**
- It is better if we calculate the WOE before splitting our training data into training and test because we will use same WOEs for the target data set and the more data we have to calculate them, the better it is.
- Below, we will WOE encode our only categorical variable neighbourhood_cleansed manually.
(We used the following guide for the WOE encoding: https://www.listendata.com/2019/08/WOE-IV-Continuous-Dependent.html#comment-form)
"""

x = target['neighbourhood_cleansed'].nunique()
y = train['neighbourhood_cleansed'].nunique()  #no differences in neighbourhood_cleansed
z = target['neighbourhood_cleansed'].isin(train['neighbourhood_cleansed']).sum()
print('There are {} unique x in the target set.'.format(x))
print('There are {} unique y in the train set.'.format(y))
print('{} of {} listings have z that we know form the training set.'.format(z, len(target)))

x = target['property_type'].nunique()
y = train['property_type'].nunique()  #4 extra categories in property type in the training data set
z = target['property_type'].isin(train['property_type']).sum()
print('There are {} unique x in the target set.'.format(x))
print('There are {} unique y in the train set.'.format(y))
print('{} of {} listings have z that we know form the training set.'.format(z, len(target)))

x = target['room_type'].nunique()
y = train['room_type'].nunique()  #no differences in room_type
z = target['room_type'].isin(train['room_type']).sum()
print('There are {} unique x in the target set.'.format(x))
print('There are {} unique y in the train set.'.format(y))
print('{} of {} listings have z that we know form the training set.'.format(z, len(target)))

x = target['cancellation_policy'].nunique()
y = train['cancellation_policy'].nunique()  #2 extra categories in cancellation policy in the training data set
z = target['cancellation_policy'].isin(train['cancellation_policy']).sum()
print('There are {} unique x in the target set.'.format(x))
print('There are {} unique y in the train set.'.format(y))
print('{} of {} listings have z that we know form the training set.'.format(z, len(target)))

train.info()

train_copy = train.copy()           #in case of errors below

train_copy2 = train_copy.copy()
#train = train_copy2

#train = train_copy

train.groupby(['neighbourhood_cleansed'])["price"].apply(lambda train : train.min())

train.groupby(['neighbourhood_cleansed'])["price"].apply(lambda train : train.max())

train.groupby(['neighbourhood_cleansed'])["price"].apply(lambda train : train.sum())

train.neighbourhood_cleansed.value_counts()

print(len(train.neighbourhood_cleansed))
Obs=train.neighbourhood_cleansed.value_counts()  /len(train.neighbourhood_cleansed)
Obs.head()

Y=train.groupby(['neighbourhood_cleansed'])["price"].apply(lambda train : train.sum()) / train.price.sum()
Y.head()

WOE = np.log(Y/Obs)*Y
WOE = pd.DataFrame(WOE, columns=['WOE_encodings'])
WOE.head()

train['neighbourhood_cleansed'].value_counts()

train_copy['neighbourhood_cleansed'].value_counts()

train['neighbourhood'].value_counts()

train['neighbourhood_cleansed'] = train['neighbourhood_cleansed'].map(WOE['WOE_encodings']) #We transfer the WOE encodings.
target['neighbourhood_cleansed'] = target['neighbourhood_cleansed'].map(WOE['WOE_encodings'])

train['neighbourhood_cleansed'].value_counts() #confirmation that it worked.

train['neighbourhood_cleansed'] = train['neighbourhood_cleansed'].astype('float32')
target['neighbourhood_cleansed'] = target['neighbourhood_cleansed'].astype('float32')

sns.distplot(train.neighbourhood_cleansed);

neighbourhood_cleansed_encoded = pd.concat([train.price, train.neighbourhood_cleansed], axis=1)
neighbourhood_cleansed_encoded.corr()["price"].sort_values(ascending=False)

"""- We check the same for the neighbourhood variable."""

print(len(train.neighbourhood))
Obs=train.neighbourhood.value_counts()  /len(train.neighbourhood)
Obs

Y=train.groupby(['neighbourhood'])["price"].apply(lambda train : train.sum()) / train.price.sum()
Y.head()

WOE = np.log(Y/Obs)*Y
WOE = pd.DataFrame(WOE, columns=['WOE_encodings'])
WOE.head()

train['neighbourhood'].value_counts()

train_copy['neighbourhood'].value_counts()

train['neighbourhood'].value_counts()

train['neighbourhood'] = train['neighbourhood'].map(WOE['WOE_encodings']) #We transfer the WOE encodings.
target['neighbourhood'] = target['neighbourhood'].map(WOE['WOE_encodings'])

train['neighbourhood'].value_counts() #confirmation that it worked.

train['neighbourhood'] = train['neighbourhood'].astype('float32')
target['neighbourhood'] = target['neighbourhood'].astype('float32')

#num_train_X['neighbourhood_cleansed'] = num_train_X['neighbourhood_cleansed'].astype('float32')
#num_test_X['neighbourhood_cleansed'] = num_test_X['neighbourhood_cleansed'].astype('float32')
#text_train_X['neighbourhood_cleansed'] = text_train_X['neighbourhood_cleansed'].astype('float32')
#text_test_X['neighbourhood_cleansed'] = text_test_X['neighbourhood_cleansed'].astype('float32')

sns.distplot(train.neighbourhood);

df = pd.concat([train.price, train.neighbourhood_cleansed,train.neighbourhood], axis=1)
df.corr()["price"].sort_values(ascending=False)

df.corr()

"""So, as we expected the adjusted neighbourhood variable is more predictive and we will use only that one from the neighbourhood data because it correlates with the other neighbourhood variable 0.79, which might have bring the issue of multicollinearity.

- Now, we will WOE encode the cancellation policy variable.
"""

print(len(train.cancellation_policy))
Obs=train.cancellation_policy.value_counts()  /len(train.cancellation_policy)
Obs

Y=train.groupby(['cancellation_policy'])["price"].apply(lambda train : train.sum()) / train.price.sum()
Y

WOE = np.log(Y/Obs)*Y
WOE = pd.DataFrame(WOE, columns=['WOE_encodings'])
WOE

train['cancellation_policy'].value_counts()

target['cancellation_policy'].value_counts()

train['cancellation_policy'] = train['cancellation_policy'].map(WOE['WOE_encodings']) #We transfer the WOE encodings.
target['cancellation_policy'] = target['cancellation_policy'].map(WOE['WOE_encodings'])

train['cancellation_policy'].value_counts() #confirmation that it worked.

train['cancellation_policy'] = train['cancellation_policy'].astype('float32')
target['cancellation_policy'] = target['cancellation_policy'].astype('float32')

sns.distplot(train.cancellation_policy);

df = pd.concat([train.price, train.neighbourhood_cleansed, train.neighbourhood,
                                           train.cancellation_policy], axis=1)
df.corr()["price"].sort_values(ascending=False)

"""- Cancellation policy turns out to be less predictive of the price than the neighbourhood variable which we excluded but still would help to include in the model."""

train.info()

"""- We check the same for the property type, room type, bed type, experiences_offered."""

train.groupby(['property_type'])["price"].apply(lambda train : train.min())

print(len(train.property_type))
Obs=train.property_type.value_counts()  /len(train.property_type)
Obs.head()

Y=train.groupby(['property_type'])["price"].apply(lambda train : train.sum()) / train.price.sum()
Y.head()

WOE = np.log(Y/Obs)*Y
WOE = pd.DataFrame(WOE, columns=['WOE_encodings'])
WOE.head()

train['property_type'] = train['property_type'].map(WOE['WOE_encodings']) #We transfer the WOE encodings.
target['property_type'] = target['property_type'].map(WOE['WOE_encodings'])

train['property_type'].value_counts() #confirmation that it worked.

train['property_type'] = train['property_type'].astype('float32')
target['property_type'] = target['property_type'].astype('float32')

sns.distplot(train.property_type);

df = pd.concat([train.price, train.neighbourhood_cleansed, train.neighbourhood,
                                           train.cancellation_policy, train.property_type], axis=1)
df.corr()["price"].sort_values(ascending=False)

df.corr()

"""We decided to include the property type because it has 0.10 correlation with the price but higher with the other variables so we think that it might help a little for us to predict the price more accurately."""

train.groupby(['room_type'])["price"].apply(lambda train : train.min())

print(len(train.room_type))
Obs=train.room_type.value_counts()  /len(train.room_type) #running everything again until here
Obs.head()

Y=train.groupby(['room_type'])["price"].apply(lambda train : train.sum()) / train.price.sum()
Y.head()

WOE = np.log(Y/Obs)*Y
WOE = pd.DataFrame(WOE, columns=['WOE_encodings'])
WOE.head()

train.room_type.value_counts()

train['room_type'] = train['room_type'].map(WOE['WOE_encodings']) #We transfer the WOE encodings.
target['room_type'] = target['room_type'].map(WOE['WOE_encodings'])

train['room_type'].value_counts() #confirmation that it worked.

train['room_type'] = train['room_type'].astype('float32')
target['room_type'] = target['room_type'].astype('float32')

sns.distplot(train.room_type);

df = pd.concat([train.price, train.neighbourhood_cleansed, train.neighbourhood,
                                           train.cancellation_policy, train.property_type,
               train.room_type], axis=1)
df.corr()["price"].sort_values(ascending=False)

train.room_type.value_counts() #entire 60 corr, private -60 corr, hotel 0.5 corr, shared -0.6 corr with the price

df = pd.concat([train.accommodates, train.beds, train.bedrooms,
                                           train.bathrooms, train.guests_included,
                train.price, train.neighbourhood_cleansed, train.neighbourhood,
                                           train.cancellation_policy, train.property_type,
               train.room_type
               ], axis=1)
df.corr()

"""Because it seems like the room type variable gives us information about an entire apartment being more expensive than a private room and because it doesn't correlate with our property feature variables extremely, we decide to use this variable. We hope our results won't be driven highly because of room types."""

train.groupby(['bed_type'])["price"].apply(lambda train : train.min())

print(len(train.bed_type))
Obs=train.bed_type.value_counts()  /len(train.bed_type)
Obs.head()

Y=train.groupby(['bed_type'])["price"].apply(lambda train : train.sum()) / train.price.sum()
Y.head()

WOE = np.log(Y/Obs)*Y
WOE = pd.DataFrame(WOE, columns=['WOE_encodings'])
WOE.head()

train.bed_type.value_counts()

train['bed_type'] = train['bed_type'].map(WOE['WOE_encodings']) #We transfer the WOE encodings.
target['bed_type'] = target['bed_type'].map(WOE['WOE_encodings'])

train['bed_type'].value_counts() #confirmation that it worked.

train['bed_type'] = train['bed_type'].astype('float32')
target['bed_type'] = target['bed_type'].astype('float32')

sns.distplot(train.bed_type);

df = pd.concat([train.price, train.neighbourhood_cleansed, train.neighbourhood,
                                           train.cancellation_policy, train.property_type,
               train.bed_type], axis=1)
df.corr()["price"].sort_values(ascending=False)

train.bed_type.value_counts() #entire 60 corr, private -60 corr, hotel 0.5 corr, shared -0.6 corr with the price

df = pd.concat([train.accommodates, train.beds, train.bedrooms,
                                           train.bathrooms, train.guests_included,
                train.price, train.neighbourhood_cleansed, train.neighbourhood,
                                           train.cancellation_policy, train.property_type,
               train.bed_type
               ], axis=1)
df.corr()



train.groupby(['experiences_offered'])["price"].apply(lambda train : train.mean())

train.experiences_offered.value_counts()

print(len(train.experiences_offered))
Obs=train.experiences_offered.value_counts()  /len(train.experiences_offered)
Obs.head()

Y=train.groupby(['experiences_offered'])["price"].apply(lambda train : train.sum()) / train.price.sum()
Y.head()

WOE = np.log(Y/Obs)*Y
WOE = pd.DataFrame(WOE, columns=['WOE_encodings'])
WOE.head()

train['experiences_offered'] = train['experiences_offered'].map(WOE['WOE_encodings']) #We transfer the WOE encodings.
target['experiences_offered'] = target['experiences_offered'].map(WOE['WOE_encodings'])

train['experiences_offered'].value_counts() #confirmation that it worked.

train['experiences_offered'] = train['experiences_offered'].astype('float32')
target['experiences_offered'] = target['experiences_offered'].astype('float32')

sns.distplot(train.experiences_offered);

df = pd.concat([train.price, train.neighbourhood_cleansed, train.neighbourhood,
                                           train.cancellation_policy, train.property_type,
               train.room_type, train.bed_type, train.experiences_offered], axis=1)
df.corr()["price"].sort_values(ascending=False)

"""- As expected, this variable isn't predictive of the price."""

target.head()

train_copy = train.copy()           #in case of errors below

train_copy2 = train_copy.copy()
#train = train_copy2

#train = train_copy

train.describe()

with open('train_copy1.pkl','wb') as path_name: #this pickle file was created at this point of notebook when rerunning
    pickle.dump(train_copy, path_name)

with open('train_copy1.pkl','rb') as path_name: #loading the file
    train_copy1 = pickle.load(path_name)

train_copy1.describe()

"""### Data Partitioning"""

train['neighbourhood_cleansed'].value_counts()

#reviews['listing_id'].value_counts()

train_structured_data = ['accommodates', 'guests_included', 'bathrooms', 'bedrooms', 'beds', 'latitude',
                              'longitude','neighbourhood_cleansed','host_total_listings_count',
                         'room_type', 'cancellation_policy', 'property_type', 'price']
target_structured_data = ['accommodates', 'guests_included', 'bathrooms', 'bedrooms', 'beds', 'latitude',
                              'longitude','neighbourhood_cleansed','host_total_listings_count',
                         'room_type', 'cancellation_policy', 'property_type',]

num_train = train[train_structured_data].copy()
num_target = target[target_structured_data].copy()


train_text_data = ['name', 'summary', 'space', 'description', 'neighbourhood_overview', 'transit','house_rules','amenities','price']
target_text_data = ['name', 'summary', 'space', 'description', 'neighbourhood_overview', 'transit','house_rules','amenities']

text_train = train[train_text_data].copy()
text_target = target[target_text_data].copy()


train_image_data = ['picture_url','price']
target_image_data = ['picture_url']

image_train = train[train_image_data].copy()
image_target = target[target_image_data].copy()

train_copy = train.copy()           #in case of errors below

train_copy2 = train_copy.copy()
#train = train_copy2

#train = train_copy

reviews.info()

len(reviews)

from sklearn.model_selection import train_test_split

X = num_train.drop('price', axis = 1)
y = num_train['price']

num_train_X, num_test_X, num_train_y, num_test_y = train_test_split(X, y, test_size = 0.3, random_state = seed)
print('Training sample contains: {} samples and {} variables'.format(num_train_X.shape[0], num_train_X.shape[1]))
print('Test sample contains: {} samples and {} variables'.format(num_test_X.shape[0], num_test_X.shape[1]))

# split data     #this was just a template to split the data
#from sklearn import model_selection
#df_train_fin, df_val_fin = model_selection.train_test_split(df_train_new, test_size=0.20, random_state=42)

X = text_train.drop('price', axis = 1)
y = text_train['price']

y

text_train_X, text_test_X, text_train_y, text_test_y = train_test_split(X, y, test_size = 0.3,random_state = seed)
print('Training sample contains: {} samples and {} variables'.format(text_train_X.shape[0], text_train_X.shape[1]))
print('Test sample contains: {} samples and {} variables'.format(text_test_X.shape[0], text_test_X.shape[1]))

num_train_y = pd.DataFrame(num_train_y, columns=['price']) #named the price column as price, before it was missing
num_test_y = pd.DataFrame(num_test_y, columns=['price'])
text_train_y = pd.DataFrame(text_train_y, columns=['price'])
text_test_y = pd.DataFrame(text_test_y, columns=['price'])

num_train_X_copy = num_train_X.copy()       #in case of errors below
num_test_X_copy = num_test_X.copy()

num_train_y_copy = num_train_y.copy()
num_test_y_copy = num_test_y.copy()

text_train_X_copy = text_train_X.copy()
text_test_X_copy = text_test_X.copy()

text_train_y_copy = text_train_y.copy()
text_test_y_copy = text_test_y.copy()

train_copy2 = train_copy.copy()
#train = train_copy2

#train = train_copy

with open('num_train_X1.pkl','wb') as path_name: #these pickle files were created at this point of notebook when rerunning
    pickle.dump(num_train_X, path_name)

with open('num_test_X1.pkl','wb') as path_name:
    pickle.dump(num_test_X, path_name)

with open('num_train_y1.pkl','wb') as path_name:
    pickle.dump(num_train_y, path_name)

with open('num_test_y1.pkl','wb') as path_name:
    pickle.dump(num_test_y, path_name)

with open('text_train_X1.pkl','wb') as path_name:
    pickle.dump(text_train_X, path_name)

with open('text_test_X1.pkl','wb') as path_name:
    pickle.dump(text_test_X, path_name)

with open('text_train_y1.pkl','wb') as path_name:
    pickle.dump(text_train_y, path_name)

with open('text_test_y1.pkl','wb') as path_name:
    pickle.dump(text_test_y, path_name)

with open('train_copy1.pkl','wb') as path_name:
    pickle.dump(train_copy, path_name)

#with open('train_copy1.pkl','rb') as path_name: #loading the file
 #   train_copy1 = pickle.load(path_name)

"""### Scalization"""

from sklearn.preprocessing import MinMaxScaler
minmaxscaler = MinMaxScaler()

from sklearn.model_selection import train_test_split

from datetime import timedelta
now = pd.Timestamp('now')

X_train.head()

X_test.head()

structured_data = ['accommodates', 'guests_included', 'bathrooms', 'bedrooms', 'beds', 'latitude',
                              'longitude','neighbourhood_cleansed','host_total_listings_count',
                  'room_type', 'cancellation_policy', 'property_type']


num_train_X = num_train_X.copy()
num_test_X = num_test_X.copy()
target = target.copy()


num_train_X[structured_data] = minmaxscaler.fit_transform(num_train_X[structured_data])

num_test_X[structured_data] = minmaxscaler.transform(num_test_X[structured_data])

target[structured_data] = minmaxscaler.transform(target[structured_data])


num_train_X.head()

price = ['price']


num_train_y = num_train_y.copy()
num_test_y = num_test_y.copy()

num_train_y[price] = minmaxscaler.fit_transform(num_train_y[price])

num_test_y[price] = minmaxscaler.transform(num_test_y[price])

print(num_train_y.head())



text_train_y = text_train_y.copy()
text_test_y = text_test_y.copy()

text_train_y[price] = minmaxscaler.fit_transform(text_train_y[price])

text_test_y[price] = minmaxscaler.transform(text_test_y[price])

print(text_train_y.head())

num_train_X_copy = num_train_X.copy()       #in case of errors below
num_test_X_copy = num_test_X.copy()

num_train_y_copy = num_train_y.copy()
num_test_y_copy = num_test_y.copy()

text_train_X_copy = text_train_X.copy()
text_test_X_copy = text_test_X.copy()

text_train_y_copy = text_train_y.copy()
text_test_y_copy = text_test_y.copy()

target_copy = target.copy()

"""### Normalization"""

num_train_X.describe()

num_test_X.describe()

from scipy import stats
from statsmodels.graphics.gofplots import qqplot

num_train_X_copy = num_train_X.copy()       #in case of errors below
num_test_X_copy = num_test_X.copy()

num_train_y_copy = num_train_y.copy()
num_test_y_copy = num_test_y.copy()

text_train_X_copy = text_train_X.copy()
text_test_X_copy = text_test_X.copy()

text_train_y_copy = text_train_y.copy()
text_test_y_copy = text_test_y.copy()

bc_fitted_feature, bc_fitted_lambda = stats.boxcox(num_train_X.accommodates+1)
num_train_X.accommodates =  bc_fitted_feature

bc_fitted_feature1, bc_fitted_lambda1 = stats.boxcox(num_test_X.accommodates+1)
num_test_X.accommodates =  bc_fitted_feature1

bc_fitted_feature2, bc_fitted_lambda2 = stats.boxcox(target.accommodates+1)
target.accommodates =  bc_fitted_feature2

bc_fitted_feature, bc_fitted_lambda = stats.boxcox(num_train_X.guests_included+1)
num_train_X.guests_included =  bc_fitted_feature

bc_fitted_feature1, bc_fitted_lambda1 = stats.boxcox(num_test_X.guests_included+1)
num_test_X.guests_included =  bc_fitted_feature1

bc_fitted_feature2, bc_fitted_lambda2 = stats.boxcox(target.guests_included+1)
target.guests_included =  bc_fitted_feature2

bc_fitted_feature, bc_fitted_lambda = stats.boxcox(num_train_X.bathrooms+1)
num_train_X.bathrooms =  bc_fitted_feature

bc_fitted_feature1, bc_fitted_lambda1 = stats.boxcox(num_test_X.bathrooms+1)
num_test_X.bathrooms =  bc_fitted_feature1

bc_fitted_feature2, bc_fitted_lambda2 = stats.boxcox(target.bathrooms+1)
target.bathrooms =  bc_fitted_feature2

bc_fitted_feature, bc_fitted_lambda = stats.boxcox(num_train_X.bedrooms+1)
num_train_X.bedrooms =  bc_fitted_feature

bc_fitted_feature1, bc_fitted_lambda1 = stats.boxcox(num_test_X.bedrooms+1)
num_test_X.bedrooms =  bc_fitted_feature1

bc_fitted_feature2, bc_fitted_lambda2 = stats.boxcox(target.bedrooms+1)
target.bedrooms =  bc_fitted_feature2

bc_fitted_feature, bc_fitted_lambda = stats.boxcox(num_train_X.beds+1)
num_train_X.beds =  bc_fitted_feature

bc_fitted_feature1, bc_fitted_lambda1 = stats.boxcox(num_test_X.beds+1)
num_test_X.beds =  bc_fitted_feature1

bc_fitted_feature2, bc_fitted_lambda2 = stats.boxcox(target.beds+1)
target.beds =  bc_fitted_feature2

bc_fitted_feature, bc_fitted_lambda = stats.boxcox(num_train_X.latitude+1)
num_train_X.latitude =  bc_fitted_feature

bc_fitted_feature1, bc_fitted_lambda1 = stats.boxcox(num_test_X.latitude+1)
num_test_X.latitude =  bc_fitted_feature1

bc_fitted_feature2, bc_fitted_lambda2 = stats.boxcox(target.latitude+1)
target.latitude =  bc_fitted_feature2

bc_fitted_feature, bc_fitted_lambda = stats.boxcox(num_train_X.longitude+1)
num_train_X.longitude =  bc_fitted_feature

bc_fitted_feature1, bc_fitted_lambda1 = stats.boxcox(num_test_X.longitude+1)
num_test_X.longitude =  bc_fitted_feature1

bc_fitted_feature2, bc_fitted_lambda2 = stats.boxcox(target.longitude+1)
target.longitude =  bc_fitted_feature2

bc_fitted_feature, bc_fitted_lambda = stats.boxcox(num_train_X.neighbourhood_cleansed+1)
num_train_X.neighbourhood_cleansed =  bc_fitted_feature

bc_fitted_feature1, bc_fitted_lambda1 = stats.boxcox(num_test_X.neighbourhood_cleansed+1)
num_test_X.neighbourhood_cleansed =  bc_fitted_feature1

bc_fitted_feature2, bc_fitted_lambda2 = stats.boxcox(target.neighbourhood_cleansed+1)
target.neighbourhood_cleansed =  bc_fitted_feature2

bc_fitted_feature, bc_fitted_lambda = stats.boxcox(num_train_X.host_total_listings_count+1)
num_train_X.host_total_listings_count =  bc_fitted_feature

bc_fitted_feature1, bc_fitted_lambda1 = stats.boxcox(num_test_X.host_total_listings_count+1)
num_test_X.host_total_listings_count =  bc_fitted_feature1

bc_fitted_feature2, bc_fitted_lambda2 = stats.boxcox(target.host_total_listings_count+1)
target.host_total_listings_count =  bc_fitted_feature2

bc_fitted_feature, bc_fitted_lambda = stats.boxcox(num_train_X.room_type+1)
num_train_X.room_type =  bc_fitted_feature

bc_fitted_feature1, bc_fitted_lambda1 = stats.boxcox(num_test_X.room_type+1)
num_test_X.room_type =  bc_fitted_feature1

bc_fitted_feature2, bc_fitted_lambda2 = stats.boxcox(target.room_type+1)
target.room_type =  bc_fitted_feature2

bc_fitted_feature, bc_fitted_lambda = stats.boxcox(num_train_X.cancellation_policy+1)
num_train_X.cancellation_policy =  bc_fitted_feature

bc_fitted_feature1, bc_fitted_lambda1 = stats.boxcox(num_test_X.cancellation_policy+1)
num_test_X.cancellation_policy =  bc_fitted_feature1

bc_fitted_feature2, bc_fitted_lambda2 = stats.boxcox(target.cancellation_policy+1)
target.cancellation_policy =  bc_fitted_feature2

bc_fitted_feature, bc_fitted_lambda = stats.boxcox(num_train_X.property_type+1)
num_train_X.property_type =  bc_fitted_feature

bc_fitted_feature1, bc_fitted_lambda1 = stats.boxcox(num_test_X.property_type+1)
num_test_X.property_type =  bc_fitted_feature1

bc_fitted_feature2, bc_fitted_lambda2 = stats.boxcox(target.property_type+1)
target.property_type =  bc_fitted_feature2

num_train_X_copy = num_train_X.copy()       #in case of errors below
num_test_X_copy = num_test_X.copy()

num_train_y_copy = num_train_y.copy()
num_test_y_copy = num_test_y.copy()

text_train_X_copy = text_train_X.copy()
text_test_X_copy = text_test_X.copy()

text_train_y_copy = text_train_y.copy()
text_test_y_copy = text_test_y.copy()

with open('num_train_X2.pkl','wb') as path_name: #these pickle files were created at this point of notebook when rerunning
    pickle.dump(num_train_X, path_name)

with open('num_test_X2.pkl','wb') as path_name:
    pickle.dump(num_test_X, path_name)

with open('num_train_y2.pkl','wb') as path_name:
    pickle.dump(num_train_y, path_name)

with open('num_test_y2.pkl','wb') as path_name:
    pickle.dump(num_test_y, path_name)

with open('text_train_X2.pkl','wb') as path_name:
    pickle.dump(text_train_X, path_name)

with open('text_test_X2.pkl','wb') as path_name:
    pickle.dump(text_test_X, path_name)

with open('text_train_y2.pkl','wb') as path_name:
    pickle.dump(text_train_y, path_name)

with open('text_test_y2.pkl','wb') as path_name:
    pickle.dump(text_test_y, path_name)

"""## Implementation of XGB"""

import xgboost as xgb
from sklearn.model_selection import RepeatedKFold, cross_val_score
splits = 6
cv_scoring = ('neg_root_mean_squared_error')

xgb_model = xgb.XGBRegressor(
    objective = 'reg:squarederror',
    learning_rate = 0.05,
    max_depth = 10,
    min_child_weight = 1,
    subsample = .1,
    colsample_bytree = .8,
    verbosity = 1
)

xgb_rkf = RepeatedKFold(n_splits = splits, n_repeats = 10, random_state = seed) #lastlastlast one -0.15020889813701313
xgb_scores = cross_val_score(xgb_model, num_train_X, num_train_y, cv = xgb_rkf, scoring = cv_scoring)
xgb_avg_rmse = np.mean(xgb_scores)
print('XGBoost: average root mean squarred error for {} folds is {}'.format(splits, xgb_avg_rmse)) #-36 before*2#-0.15044830118616423before

# fit XGBoost and make predictions
xgb_model.fit(num_train_X, num_train_y)

# plot feature importance for XGBoost #latitude 8233 #longitude6105
xgb.plot_importance(xgb_model)
plt.show()

"""Every variable that was included in the model was free of outliers, scaled and normalized and the price variable also except the normalization. So, based on the results above XGB model seems to predict the prices mainly based on the coordinate data, which is good because they are important but maybe a little too much."""

sns.distplot(num_train_X.latitude);

sns.distplot(num_train_y.price);

target['host_total_listings_count'] = target['host_total_listings_count'].astype('float32') #fixation
target['latitude'] = target['longitude'].astype('float32')
target['accommodates'] = target['accommodates'].astype('float32')
target['beds'] = target['beds'].astype('float32')
target['bedrooms'] = target['bedrooms'].astype('float32')
target['bathrooms'] = target['bathrooms'].astype('float32')
target['guests_included'] = target['guests_included'].astype('float32')

num_train_X_copy = num_train_X.copy()       #in case of errors below
num_test_X_copy = num_test_X.copy()

num_train_y_copy = num_train_y.copy()
num_test_y_copy = num_test_y.copy()

text_train_X_copy = text_train_X.copy()
text_test_X_copy = text_test_X.copy()

text_train_y_copy = text_train_y.copy()
text_test_y_copy = text_test_y.copy()

with open('num_train_X3.pkl','wb') as path_name: #we saved the versions of the dataframes at this point of notebook
    pickle.dump(num_train_X, path_name)
with open('num_train_y3.pkl','wb') as path_name:
    pickle.dump(num_train_y, path_name)
with open('num_test_X3.pkl','wb') as path_name:
    pickle.dump(num_test_X, path_name)
with open('num_test_y3.pkl','wb') as path_name:
    pickle.dump(num_test_y, path_name)

with open('text_train_X3.pkl','wb') as path_name:
    pickle.dump(text_train_X, path_name)
with open('text_train_y3.pkl','wb') as path_name:
    pickle.dump(text_train_y, path_name)
with open('text_test_X3.pkl','wb') as path_name:
    pickle.dump(text_test_X, path_name)
with open('text_test_y3.pkl','wb') as path_name:
    pickle.dump(text_test_y, path_name)

with open('target3.pkl','wb') as path_name: #left here with the final versions of the data sets before the text analysis
    pickle.dump(target, path_name) #za

with open('train_copy1.pkl','rb') as path_name: #loading the file
    train_copy1 = pickle.load(path_name)

with open('num_train_X3.pkl','rb') as path_name:
    num_train_X = pickle.load(path_name)
with open('num_train_y3.pkl','rb') as path_name:
    num_train_y = pickle.load(path_name)
with open('num_test_X3.pkl','rb') as path_name:
    num_test_X = pickle.load(path_name)
with open('num_test_y3.pkl','rb') as path_name:
    num_test_y = pickle.load(path_name)

with open('text_train_X3.pkl','rb') as path_name:
    text_train_X = pickle.load(path_name)
with open('text_train_y3.pkl','rb') as path_name:
    text_train_y = pickle.load(path_name)
with open('text_test_X3.pkl','rb') as path_name:
    text_test_X = pickle.load(path_name)
with open('text_test_y3.pkl','rb') as path_name:
    text_test_y = pickle.load(path_name)

with open('target3.pkl','rb') as path_name:
    target = pickle.load(path_name)

"""### Preparation to the TEXT section"""

num_train_X_copy = num_train_X.copy()       #in case of errors below
num_test_X_copy = num_test_X.copy()

num_train_y_copy = num_train_y.copy()
num_test_y_copy = num_test_y.copy()

text_train_X_copy = text_train_X.copy()
text_test_X_copy = text_test_X.copy()

text_train_y_copy = text_train_y.copy()
text_test_y_copy = text_test_y.copy()

num_train_X_copy2 = num_train_X.copy()       #in case of errors below
num_test_X_copy2 = num_test_X.copy()

num_train_y_copy2 = num_train_y.copy()
num_test_y_copy2 = num_test_y.copy()

text_train_X_copy2 = text_train_X.copy()
text_test_X_copy2 = text_test_X.copy()

text_train_y_copy2 = text_train_y.copy()
text_test_y_copy2 = text_test_y.copy()

text_features = ['name', 'summary', 'space', 'description', 'neighbourhood_overview', 'transit', 'house_rules', 'amenities']

numerical_features = ['accommodates','guests_included','bathrooms','bedrooms','beds','longitude','latitude',
                      'neighbourhood_cleansed', 'host_total_listings_count','room_type','cancellation_policy',
                     'property_type']

target_numerical_data = target[numerical_features]
target_numerical_data

target_text_data = target[text_features] #at this point these are uncleaned.
target_text_data

num_train_X_copy = num_train_X.copy()       #in case of errors below
num_test_X_copy = num_test_X.copy()

num_train_y_copy = num_train_y.copy()
num_test_y_copy = num_test_y.copy()

text_train_X_copy = text_train_X.copy()
text_test_X_copy = text_test_X.copy()

text_train_y_copy = text_train_y.copy()
text_test_y_copy = text_test_y.copy()

num_train_X_copy2 = num_train_X.copy()       #in case of errors below
num_test_X_copy2 = num_test_X.copy()

num_train_y_copy2 = num_train_y.copy()
num_test_y_copy2 = num_test_y.copy()

text_train_X_copy2 = text_train_X.copy()
text_test_X_copy2 = text_test_X.copy()

text_train_y_copy2 = text_train_y.copy()
text_test_y_copy2 = text_test_y.copy()

"""- Another approach we might try based on the ADAMS 2020 demo notebook:

# TEXT
"""

with open('num_train_X.pkl3','rb') as path_name: #we reload the versions of the dataframes before starting text analysis
    num_train_X = pickle.load(path_name)
with open('num_train_y.pkl3','rb') as path_name:
    num_train_y = pickle.load(path_name)
with open('num_test_X.pkl3','rb') as path_name:
    num_test_X = pickle.load(path_name)
with open('num_test_y.pkl3','rb') as path_name:
    num_test_y = pickle.load(path_name)

with open('text_train_X.pkl3','rb') as path_name:
    text_train_X = pickle.load(path_name)
with open('text_train_y.pkl3','rb') as path_name:
    text_train_y = pickle.load(path_name)
with open('text_test_X.pkl3','rb') as path_name:
    text_test_X = pickle.load(path_name)
with open('text_test_y.pkl3','rb') as path_name:
    text_test_y = pickle.load(path_name)

num_train_X_copy = num_train_X.copy()       #in case of errors below
num_test_X_copy = num_test_X.copy()

num_train_y_copy = num_train_y.copy()
num_test_y_copy = num_test_y.copy()

text_train_X_copy = text_train_X.copy()
text_test_X_copy = text_test_X.copy()

text_train_y_copy = text_train_y.copy()
text_test_y_copy = text_test_y.copy()

num_train_X_copy2 = num_train_X.copy()       #in case of errors below
num_test_X_copy2 = num_test_X.copy()

num_train_y_copy2 = num_train_y.copy()
num_test_y_copy2 = num_test_y.copy()

text_train_X_copy2 = text_train_X.copy()
text_test_X_copy2 = text_test_X.copy()

text_train_y_copy2 = text_train_y.copy()
text_test_y_copy2 = text_test_y.copy()

def get_wordnet_pos(word):
    """Map POS tag to first character for lemmatization"""
    tag = nltk.pos_tag([word])[0][1][0].upper()
    tag_dict = {"J": wordnet.ADJ,
                "N": wordnet.NOUN,
                "V": wordnet.VERB,
                "R": wordnet.ADV}
    return tag_dict.get(tag, wordnet.NOUN)


def clean_articles(text):
    lemmatizer = WordNetLemmatizer()
     # remove newlines
    review_text = text.replace("\n", " ")
    review_text = review_text.replace("\xa0", " ")
    # remove html content
    review_text = BeautifulSoup(review_text, "html.parser").get_text()
    # remove URLs
    review_text = re.sub(r'http\S+', '', review_text)
    review_text = re.sub(r'www\.\S+', '', review_text)
    # remove bibliographies (can be at start or end):
    # assuming bibliography is shorter than text:
    test_split = str(review_text).split("Bibliography")
    review_text = test_split[np.where(np.array([len(x) for x in test_split]) == max([len(x) for x in test_split]))[0].tolist().pop()]
    # remove non-alphabetic characters
    review_text = re.sub("[^a-zA-Z]", " ", review_text)
    # fix contractions
    review_text = re.sub(r"['’]ll", " will", review_text)
    review_text = re.sub(r"['’]ve", " have", review_text)
    review_text = re.sub(r"['’]re", " are", review_text)
    review_text = re.sub(r"['’]d", " would", review_text)
    review_text = re.sub(r"['’]m", " am", review_text)
    review_text = re.sub(r"(?i)there['’]s", "there is", review_text)
    review_text = re.sub(r"(?i)that['’]s", "that is", review_text)
    review_text = re.sub(r"(?i)it['’]s", "it is", review_text)
    review_text = re.sub(r"(?i)he['’]s", "he is", review_text)
    review_text = re.sub(r"can['’]t", "cannot", review_text)
    review_text = re.sub(r"won['’]t", "will not", review_text)
    review_text = re.sub(r"n['’]t", " not", review_text)
    review_text = review_text.replace("gonna", "going to")
    review_text = review_text.replace("wanna", "want to")
    review_text = review_text.replace("gotta", "got to")
    # tokenize the sentences
    words = word_tokenize(review_text.lower())
    # filter stopwords
    words = [w for w in words if w not in stopwords.words("english")]
    # lemmatize each word to its lemma
    lemma_words = [lemmatizer.lemmatize(i, get_wordnet_pos(i)) for i in words]
    return lemma_words

text_train_X_cleaned = text_train_X
text_test_X_cleaned = text_test_X

text_features

text_train_X_cleaned[text_features] = text_train_X_cleaned[text_features].replace(np.nan, '')
text_test_X_cleaned[text_features] = text_test_X_cleaned[text_features].replace(np.nan, '')

"""- We first start with the cleaning of the training data set."""

text_train_X_cleaned['name'] = text_train_X['name'].apply(clean_articles)

text_train_X_cleaned['summary'] = text_train_X['summary'].apply(clean_articles)

text_train_X_cleaned['space'] = text_train_X['space'].apply(clean_articles)

text_train_X_cleaned['description'] = text_train_X['description'].apply(clean_articles)

text_train_X_cleaned['neighbourhood_overview'] = text_train_X['neighbourhood_overview'].apply(clean_articles)

text_train_X_cleaned['transit'] = text_train_X['transit'].apply(clean_articles)

text_train_X_cleaned['house_rules'] = text_train_X['house_rules'].apply(clean_articles)

text_train_X_cleaned['amenities'] = text_train_X['amenities'].apply(clean_articles)

with open('text_train_X_cleaned1.pkl','wb') as path_name: #saved at this point of notebook
    pickle.dump(text_train_X_cleaned, path_name)

with open('text_train_X_cleaned2.pkl','wb') as path_name: #saved at this point of notebook
    pickle.dump(text_train_X_cleaned, path_name)

with open('text_train_X_cleaned1.pkl','rb') as path_name: #reload the relevant data frame
    text_train_X_cleaned = pickle.load(path_name)

text_train_X_cleaned

"""- We then continue with the cleaning of the test data set."""

text_test_X_cleaned['name'] = text_test_X['name'].apply(clean_articles)

text_test_X_cleaned['summary'] = text_test_X['summary'].apply(clean_articles)

text_test_X_cleaned['space'] = text_test_X['space'].apply(clean_articles)

text_test_X_cleaned['description'] = text_test_X['description'].apply(clean_articles)

text_test_X_cleaned['neighbourhood_overview'] = text_test_X['neighbourhood_overview'].apply(clean_articles)

text_test_X_cleaned['transit'] = text_test_X['transit'].apply(clean_articles)

text_test_X_cleaned['house_rules'] = text_test_X['house_rules'].apply(clean_articles)

text_test_X_cleaned['amenities'] = text_test_X['amenities'].apply(clean_articles)

"""- We save the cleaned texts from our training and the test set and we create copies of them in case we will need the initial versions of them."""

with open('text_test_X_cleaned1.pkl','wb') as path_name: #saved at this point of notebook
    pickle.dump(text_test_X_cleaned, path_name)

with open('text_test_X_cleaned2.pkl','wb') as path_name: #saved at this point of notebook
    pickle.dump(text_test_X_cleaned, path_name)

with open('text_test_X_cleaned1.pkl','rb') as path_name: #reload the relevant data frame
    text_test_X_cleaned = pickle.load(path_name)

text_test_X_cleaned

text_train_cleaned_copy = text_train_X_cleaned.copy()        #in case of errors below

text_train_cleaned_copy2 = text_train_X_cleaned.copy()

#text_train_cleaned = text_train_cleaned_copy2

text_test_cleaned_copy = text_test_X_cleaned.copy()        #in case of errors below

text_test_cleaned_copy2 = text_test_X_cleaned.copy()

#text_test_cleaned = text_test_cleaned_copy2

"""- Now we clean the text columns of our target data set."""

target_numerical_data #a reminder of our target numerical data although at this point it still needs some scaling

print(target.isnull().sum())

"""**A remark:**
- Since the neighbourhood_overview, house_rules and the transit variables have too many missings we might not use them in predicting the final prices.
"""

target[text_features] = target[text_features].replace(np.nan, '')
target[text_features] = target[text_features].replace(np.nan, '')

target_cleaned = target

target_cleaned['name'] = target['name'].apply(clean_articles)

with open('target_cleaned1.pkl','wb') as path_name: #saved at this point of notebook
    pickle.dump(target_cleaned, path_name)

target_cleaned['summary'] = target['summary'].apply(clean_articles)

with open('target_cleaned1.pkl','wb') as path_name: #saved at this point of notebook
    pickle.dump(target_cleaned, path_name)

target_cleaned['space'] = target['space'].apply(clean_articles)

with open('target_cleaned1.pkl','wb') as path_name: #saved at this point of notebook
    pickle.dump(target_cleaned, path_name)

target_cleaned['description'] = target['description'].apply(clean_articles)

with open('target_cleaned1.pkl','wb') as path_name: #saved at this point of notebook
    pickle.dump(target_cleaned, path_name)

target_cleaned['neighbourhood_overview'] = target['neighbourhood_overview'].apply(clean_articles)

with open('target_cleaned1.pkl','wb') as path_name: #saved at this point of notebook
    pickle.dump(target_cleaned, path_name)

target_cleaned['transit'] = target['transit'].apply(clean_articles)

with open('target_cleaned1.pkl','wb') as path_name: #saved at this point of notebook
    pickle.dump(target_cleaned, path_name)

target_cleaned['house_rules'] = target['house_rules'].apply(clean_articles)

with open('target_cleaned1.pkl','wb') as path_name: #saved at this point of notebook
    pickle.dump(target_cleaned, path_name)

target_cleaned['amenities'] = target['amenities'].apply(clean_articles)

with open('target_cleaned1.pkl','wb') as path_name: #saved at this point of notebook
    pickle.dump(target_cleaned, path_name)

with open('target_cleaned2.pkl','wb') as path_name: #saved at this point of notebook
    pickle.dump(target_cleaned, path_name)

with open('target_cleaned1.pkl','rb') as path_name: #reload the relevant data frame
    target_text_cleaned = pickle.load(path_name)

target_text_cleaned= target_text_cleaned[text_features]
target_text_cleaned

target_text_cleaned_copy = target_text_cleaned.copy()        #in case of errors below

target_text_cleaned_copy2 = target_text_cleaned.copy()

#target_text_cleaned = target_text_cleaned_copy2

target_text_cleaned_copy = target_text_cleaned.copy()        #in case of errors below

target_text_cleaned_copy2 = target_text_cleaned.copy()

#target_text_cleaned = target_text_cleaned_copy2

"""- Now, we continue with our text pre-processing and load word vectors from the Wiki2Vec pretrained model."""

# Load wiki2vec embeddings: #C:\Users\Administrator\Downloads\
start = time.time()
wiki2vec = KeyedVectors.load_word2vec_format('C:/Users/Administrator/Downloads/enwiki_20180420_100d.txt.bz2')
end = time.time()
print(f"Loaded wiki2vec embeddings in {end - start} seconds.")

wiki2vec

with open('wiki2vec1.pkl','wb') as path_name: #saved at this point of notebook
    pickle.dump(wiki2vec, path_name)

with open('wiki2vec2.pkl','wb') as path_name: #saved at this point of notebook
    pickle.dump(wiki2vec, path_name)

"""**Ideas about the modelling approach:**
-  We won't use the summary column because all or a very big portion of the listings' descriptions includes the summary in the beginning.
- We thought of predicting the price based on text data in a neural network or tree based model and then combining the price output to a new model but predictions would likely to be poor only based on text data, since they are not the main drivers but only some features to make the predictions better and slightly more accurate.
- Description and space column could be useful because it might include some details about the room or the apartment that doesn't exist in the columns we have.
- The same applies to the house rules, transit and amenities
- The name column doesn't add new information but (this part applies to all text columns) with the help of our model we might be able to infer some new information out of them that could help predict the price slightly better because of the way they are written, the words they chose and the like.
"""

#with open('text_train_X_cleaned1.pkl','rb') as path_name:
 #   text_train_X = pickle.load(path_name)

#with open('text_test_X_cleaned1.pkl','rb') as path_name:
 #   text_test_X = pickle.load(path_name)

frames = [text_train_X_cleaned.name.str.len(), text_test_X_cleaned.name.str.len()]
df1 = pd.concat(frames)
df1 = pd.DataFrame(df1, columns=['name'])

frames = [text_train_X_cleaned.summary.str.len(), text_test_X_cleaned.summary.str.len()]
df2 = pd.concat(frames)
df2 = pd.DataFrame(df2, columns=['summary'])
df2.head()

frames = [text_train_X_cleaned.space.str.len(), text_test_X_cleaned.space.str.len()]
df3 = pd.concat(frames)
df3 = pd.DataFrame(df3, columns=['space'])
df3.head()

frames = [text_train_X_cleaned.description.str.len(), text_test_X_cleaned.description.str.len()]
df4 = pd.concat(frames)
df4 = pd.DataFrame(df4, columns=['description'])
df4.head()

frames = [text_train_X_cleaned.neighbourhood_overview.str.len(), text_test_X_cleaned.neighbourhood_overview.str.len()]
df5 = pd.concat(frames)
df5 = pd.DataFrame(df5, columns=['neighbourhood_overview'])
df5.head()

frames = [text_train_X_cleaned.transit.str.len(), text_test_X_cleaned.transit.str.len()]
df6 = pd.concat(frames)
df6 = pd.DataFrame(df6, columns=['transit'])
df6.head()

frames = [text_train_X_cleaned.house_rules.str.len(), text_test_X_cleaned.house_rules.str.len()]
df7 = pd.concat(frames)
df7 = pd.DataFrame(df7, columns=['house_rules'])
df7.head()

frames = [text_train_X_cleaned.amenities.str.len(), text_test_X_cleaned.amenities.str.len()]
df8 = pd.concat(frames)
df8 = pd.DataFrame(df8, columns=['amenities'])
df8

new_df = df1.merge(df2,on='listing_id').merge(df3,on='listing_id').merge(df4,on='listing_id').merge(df5,on='listing_id').merge(df6,on='listing_id').merge(df7,on='listing_id').merge(df8,on='listing_id')

new_df.describe()

"""- We calculated the data frame above to see if we have some very long text data that could potentially cause problems to models such as LSTM but we don't observe that.
- Also, we might use the above data frame to calculate the length of the text data as a numeric data to our models.
"""

def get_embedding_matrix(tokenizer, pretrain, vocab_size):
    '''
        Helper function to construct an embedding matrix for
        the focal corpus based on some pre-trained embeddings.
    '''

    dim = 0
    if isinstance(pretrain, KeyedVectors) or isinstance(pretrain, Word2VecKeyedVectors):
        dim = pretrain.vector_size
    elif isinstance(pretrain, dict):
        dim = next(iter(pretrain.values())).shape[0]  # get embedding of an arbitrary word
    else:
        raise Exception('{} is not supported'.format(type(pretrain)))

    # Initialize embedding matrix
    emb_mat = np.zeros((vocab_size, dim))

    # There will be some words in our corpus for which we lack a pre-trained embedding.
    # In this tutorial, we will simply use a vector of zeros for such words. We also keep
    # track of the words to do some debugging if needed
    oov_words = []
    # Below we use the tokenizer object that created our task vocabulary. This is crucial to ensure
    # that the position of a words in our embedding matrix corresponds to its index in our integer
    # encoded input data
    for word, i in tokenizer.word_index.items():
        # try-catch together with a zero-initilaized embedding matrix achieves our rough fix for oov words
        try:
            emb_mat[i] = pretrain[word]
        except:
            oov_words.append(word)
    print('Created embedding matrix of shape {}'.format(emb_mat.shape))
    print('Encountered {} out-of-vocabulary words.'.format(len(oov_words)))
    return emb_mat, oov_words

"""- We thought that the fact that we do the below operation in the aggregated data set might result us getting unrealistic higher scores on the test data is fine because we will get equally biased results so we can still compare them and the opportunity to be able to get more accurate predictions in the target data set outweighted the little risk of bias mentioned."""

text_data_agg = (pd.concat([text_train_X_cleaned,text_test_X_cleaned], axis=0)) #short for "data_aggregated"
text_data_agg

max(text_train_X_cleaned.name.str.len())
max(text_train_X_cleaned.summary.str.len())
max(text_train_X_cleaned.space.str.len())
max(text_train_X_cleaned.description.str.len())
max(text_train_X_cleaned.neighbourhood_overview.str.len())
max(text_train_X_cleaned.house_rules.str.len())
max(text_train_X_cleaned.transit.str.len())
max(text_train_X_cleaned.amenities.str.len())

tokenizer_name = Tokenizer(oov_token=1, filters='!"#$%&()*+,-.:;<=>?@[\\]^`{|}~\t\n', lower=False)
tokenizer_name.fit_on_texts(text_train_X_cleaned.name)
NUM_WORDS_NAME = len(tokenizer_name.word_index) + 1

X_tr_int_name = tokenizer_name.texts_to_sequences(text_train_X_cleaned.name)
max_text_length = max([len(article) for article in X_tr_int_name])
print('The longest text in the training data set, name variable has {} words.'.format(max_text_length))

# Upper bound of the variable length for padding
MAX_NAME_LENGTH = 30
X_tr_int_pad_name = pad_sequences(X_tr_int_name, MAX_NAME_LENGTH)

# Encode and pad the test data
X_ts_int_name = tokenizer_name.texts_to_sequences(text_train_X_cleaned.name)
X_ts_int_pad_name = pad_sequences(X_ts_int_name, MAX_NAME_LENGTH)

wiki_weights_name, _ = get_embedding_matrix(tokenizer_name, wiki2vec, NUM_WORDS_NAME)


#
tokenizer_summary = Tokenizer(oov_token=1, filters='!"#$%&()*+,-.:;<=>?@[\\]^`{|}~\t\n', lower=False)
tokenizer_summary.fit_on_texts(text_train_X_cleaned.summary)
NUM_WORDS_SUMMARY = len(tokenizer_summary.word_index) + 1

X_tr_int_summary = tokenizer_summary.texts_to_sequences(text_train_X_cleaned.summary)
max_text_length = max([len(article) for article in X_tr_int_summary])
print('The longest text in the training data set, summary variable has {} words.'.format(max_text_length))

# Upper bound of the variable length for padding
MAX_SUMMARY_LENGTH = 123
X_tr_int_pad_summary = pad_sequences(X_tr_int_summary, MAX_SUMMARY_LENGTH)

# Encode and pad the test data
X_ts_int_summary = tokenizer_summary.texts_to_sequences(text_train_X_cleaned.summary)
X_ts_int_pad_summary = pad_sequences(X_ts_int_summary, MAX_SUMMARY_LENGTH)

wiki_weights_summary, _ = get_embedding_matrix(tokenizer_summary, wiki2vec, NUM_WORDS_SUMMARY)


#
tokenizer_space = Tokenizer(oov_token=1, filters='!"#$%&()*+,-.:;<=>?@[\\]^`{|}~\t\n', lower=False)
tokenizer_space.fit_on_texts(text_train_X_cleaned.space)
NUM_WORDS_SPACE = len(tokenizer_space.word_index) + 1

X_tr_int_space = tokenizer_space.texts_to_sequences(text_train_X_cleaned.space)
max_text_length = max([len(article) for article in X_tr_int_space])
print('The longest text in the training data set, space variable has {} words.'.format(max_text_length))

# Upper bound of the variable length for padding
MAX_SPACE_LENGTH = 195
X_tr_int_pad_space = pad_sequences(X_tr_int_space, MAX_SPACE_LENGTH)

# Encode and pad the test data
X_ts_int_space = tokenizer_space.texts_to_sequences(text_train_X_cleaned.space)
X_ts_int_pad_space = pad_sequences(X_ts_int_space, MAX_SPACE_LENGTH)

wiki_weights_space, _ = get_embedding_matrix(tokenizer_space, wiki2vec, NUM_WORDS_SPACE)


#
tokenizer_description = Tokenizer(oov_token=1, filters='!"#$%&()*+,-.:;<=>?@[\\]^`{|}~\t\n', lower=False)
tokenizer_description.fit_on_texts(text_train_X_cleaned.description)
NUM_WORDS_DESCRIPTION = len(tokenizer_description.word_index) + 1

X_tr_int_description = tokenizer_description.texts_to_sequences(text_train_X_cleaned.description)
max_text_length = max([len(article) for article in X_tr_int_description])
print('The longest text in the training data set, description variable has {} words.'.format(max_text_length))

# Upper bound of the variable length for padding
MAX_DESCRIPTION_LENGTH = 197
X_tr_int_pad_description = pad_sequences(X_tr_int_description, MAX_DESCRIPTION_LENGTH)

# Encode and pad the test data
X_ts_int_description = tokenizer_description.texts_to_sequences(text_train_X_cleaned.description)
X_ts_int_pad_description = pad_sequences(X_ts_int_description, MAX_DESCRIPTION_LENGTH)

wiki_weights_description, _ = get_embedding_matrix(tokenizer_description, wiki2vec, NUM_WORDS_DESCRIPTION)


#
tokenizer_neighbourhood_overview = Tokenizer(oov_token=1, filters='!"#$%&()*+,-.:;<=>?@[\\]^`{|}~\t\n', lower=False)
tokenizer_neighbourhood_overview.fit_on_texts(text_train_X_cleaned.neighbourhood_overview)
NUM_WORDS_NEIGHBOURHOOD_OVERVIEW = len(tokenizer_neighbourhood_overview.word_index) + 1

X_tr_int_neighbourhood_overview = tokenizer_neighbourhood_overview.texts_to_sequences(text_train_X_cleaned.neighbourhood_overview)
max_text_length = max([len(article) for article in X_tr_int_neighbourhood_overview])
print('The longest text in the training data set, neighbourhood_overview variable has {} words.'.format(max_text_length))

# Upper bound of the variable length for padding
MAX_NEIGHBOURHOOD_OVERVIEW_LENGTH = 191
X_tr_int_pad_neighbourhood_overview = pad_sequences(X_tr_int_neighbourhood_overview, MAX_NEIGHBOURHOOD_OVERVIEW_LENGTH)

# Encode and pad the test data
X_ts_int_neighbourhood_overview = tokenizer_neighbourhood_overview.texts_to_sequences(text_train_X_cleaned.neighbourhood_overview)
X_ts_int_pad_neighbourhood_overview = pad_sequences(X_ts_int_neighbourhood_overview, MAX_NEIGHBOURHOOD_OVERVIEW_LENGTH)

wiki_weights_neighbourhood_overview, _ = get_embedding_matrix(tokenizer_neighbourhood_overview, wiki2vec, NUM_WORDS_NEIGHBOURHOOD_OVERVIEW)


#
tokenizer_house_rules = Tokenizer(oov_token=1, filters='!"#$%&()*+,-.:;<=>?@[\\]^`{|}~\t\n', lower=False)
tokenizer_house_rules.fit_on_texts(text_train_X_cleaned.house_rules)
NUM_WORDS_HOUSE_RULES = len(tokenizer_house_rules.word_index) + 1

X_tr_int_house_rules = tokenizer_house_rules.texts_to_sequences(text_train_X_cleaned.house_rules)
max_text_length = max([len(article) for article in X_tr_int_house_rules])
print('The longest text in the training data set, house_rules variable has {} words.'.format(max_text_length))

# Upper bound of the variable length for padding
MAX_HOUSE_RULES_LENGTH = 159
X_tr_int_pad_house_rules = pad_sequences(X_tr_int_house_rules, MAX_HOUSE_RULES_LENGTH)

# Encode and pad the test data
X_ts_int_house_rules = tokenizer_house_rules.texts_to_sequences(text_train_X_cleaned.house_rules)
X_ts_int_pad_house_rules = pad_sequences(X_ts_int_house_rules, MAX_HOUSE_RULES_LENGTH)

wiki_weights_house_rules, _ = get_embedding_matrix(tokenizer_house_rules, wiki2vec, NUM_WORDS_HOUSE_RULES)


#
tokenizer_transit = Tokenizer(oov_token=1, filters='!"#$%&()*+,-.:;<=>?@[\\]^`{|}~\t\n', lower=False)
tokenizer_transit.fit_on_texts(text_train_X_cleaned.transit)
NUM_WORDS_TRANSIT = len(tokenizer_transit.word_index) + 1

X_tr_int_transit = tokenizer_transit.texts_to_sequences(text_train_X_cleaned.transit)
max_text_length = max([len(article) for article in X_tr_int_transit])
print('The longest text in the training data set, transit variable has {} words.'.format(max_text_length))

# Upper bound of the variable length for padding
MAX_TRANSIT_LENGTH = 146
X_tr_int_pad_transit = pad_sequences(X_tr_int_transit, MAX_TRANSIT_LENGTH)

# Encode and pad the test data
X_ts_int_transit = tokenizer_transit.texts_to_sequences(text_train_X_cleaned.transit)
X_ts_int_pad_transit = pad_sequences(X_ts_int_transit, MAX_TRANSIT_LENGTH)

wiki_weights_transit, _ = get_embedding_matrix(tokenizer_transit, wiki2vec, NUM_WORDS_TRANSIT)


#
tokenizer_amenities = Tokenizer(oov_token=1, filters='!"#$%&()*+,-.:;<=>?@[\\]^`{|}~\t\n', lower=False)
tokenizer_amenities.fit_on_texts(text_train_X_cleaned.amenities)
NUM_WORDS_AMENITIES = len(tokenizer_amenities.word_index) + 1

X_tr_int_amenities = tokenizer_amenities.texts_to_sequences(text_train_X_cleaned.amenities)
max_text_length = max([len(article) for article in X_tr_int_amenities])
print('The longest text in the training data set, amenities variable has {} words.'.format(max_text_length))

# Upper bound of the variable length for padding
MAX_AMENITIES_LENGTH = 200
X_tr_int_pad_amenities = pad_sequences(X_tr_int_amenities, MAX_AMENITIES_LENGTH)

# Encode and pad the test data
X_ts_int_amenities = tokenizer_amenities.texts_to_sequences(text_train_X_cleaned.amenities)
X_ts_int_pad_amenities = pad_sequences(X_ts_int_amenities, MAX_AMENITIES_LENGTH)

wiki_weights_amenities, _ = get_embedding_matrix(tokenizer_amenities, wiki2vec, NUM_WORDS_AMENITIES)

"""### Final Adjustment Before the Model Building

- First we calculate the length of the text variables.
"""

frames = [text_train_y, text_test_y]
main_df = pd.concat(frames)
main_df

final_df = new_df.merge(main_df,on='listing_id')  #we merge it with the data frame of lengths we calculated priorly.
final_df[text_features] = minmaxscaler.fit_transform(final_df[text_features])
final_df

final_df.describe()

final_df.corr()["price"].sort_values(ascending=False)

final_df = final_df.rename(columns={'name': 'name_len'})
final_df = final_df.rename(columns={'summary': 'summary_len'})
final_df = final_df.rename(columns={'space': 'space_len'})
final_df = final_df.rename(columns={'description': 'description_len'})
final_df = final_df.rename(columns={'neighbourhood_overview': 'neighbourhood_overview_len'})
final_df = final_df.rename(columns={'transit': 'transit_len'})
final_df = final_df.rename(columns={'house_rules': 'house_len'})
final_df = final_df.rename(columns={'amenities': 'amenities_len'})

"""- Based on the results above, after normalization, we include the variables that have 0.10 of correlation and higher."""

bc_fitted_feature, bc_fitted_lambda = stats.boxcox(final_df.name_len+1)
final_df.name_len =  bc_fitted_feature
bc_fitted_feature, bc_fitted_lambda = stats.boxcox(final_df.summary_len+1.2)
final_df.summary_len =  bc_fitted_feature
bc_fitted_feature, bc_fitted_lambda = stats.boxcox(final_df.space_len+1)
final_df.space_len =  bc_fitted_feature
bc_fitted_feature, bc_fitted_lambda = stats.boxcox(final_df.description_len+1)
final_df.description_len =  bc_fitted_feature
bc_fitted_feature, bc_fitted_lambda = stats.boxcox(final_df.neighbourhood_overview_len+1)
final_df.neighbourhood_overview_len =  bc_fitted_feature
bc_fitted_feature, bc_fitted_lambda = stats.boxcox(final_df.transit_len+1)
final_df.transit_len =  bc_fitted_feature
bc_fitted_feature, bc_fitted_lambda = stats.boxcox(final_df.house_len+1)
final_df.house_len =  bc_fitted_feature
bc_fitted_feature, bc_fitted_lambda = stats.boxcox(final_df.amenities_len+1)
final_df.amenities_len =  bc_fitted_feature

num_train_X = num_train_X.merge(final_df[['space_len','amenities_len','neighbourhood_overview_len','description_len','house_len']],on='listing_id')
num_test_X = num_test_X.merge(final_df[['space_len','amenities_len','neighbourhood_overview_len','description_len','house_len']],on='listing_id')
num_train_X

with open('num_train_X10.pkl','wb') as path_name: #saved at this point of notebook
    pickle.dump(num_train_X, path_name)

with open('num_test_X10.pkl','wb') as path_name: #saved at this point of notebook
    pickle.dump(num_test_X, path_name)

with open('num_train_X11.pkl','wb') as path_name: #saved at this point of notebook
    pickle.dump(num_train_X, path_name)

with open('num_test_X11.pkl','wb') as path_name: #saved at this point of notebook
    pickle.dump(num_test_X, path_name)

with open('num_train_X11.pkl','rb') as path_name: #to reload the relevant data frames from this point of notebook
    num_train_X = pickle.load(path_name)

with open('num_test_X11.pkl','rb') as path_name:
    num_test_X = pickle.load(path_name)

"""- The same process is done to the target data set."""

frames = [target_text_cleaned.space.str.len()]
df1 = pd.DataFrame(frames)
df1 = df1.transpose()

frames = [target_text_cleaned.amenities.str.len()]
df2 = pd.DataFrame(frames)
df2 = df2.transpose()

frames = [target_text_cleaned.neighbourhood_overview.str.len()]
df3 = pd.DataFrame(frames)
df3 = df3.transpose()


frames = [target_text_cleaned.description.str.len()]
df4 = pd.DataFrame(frames)
df4 = df4.transpose()


frames = [target_text_cleaned.house_rules.str.len()]
df5 = pd.DataFrame(frames)
df5 = df5.transpose()


new_df = df1.merge(df2,on='listing_id').merge(df3,on='listing_id').merge(df4,on='listing_id').merge(df5,on='listing_id')

new_df.describe()

data =['space','amenities','neighbourhood_overview','description','house_rules']
new_df = pd.DataFrame(new_df[['space','amenities','neighbourhood_overview','description','house_rules']])

new_df[data] = minmaxscaler.fit_transform(new_df[data])
new_df.describe()

new_df = new_df.rename(columns={'space': 'space_len'})
new_df = new_df.rename(columns={'description': 'description_len'})
new_df = new_df.rename(columns={'neighbourhood_overview': 'neighbourhood_overview_len'})
new_df = new_df.rename(columns={'house_rules': 'house_len'})
new_df = new_df.rename(columns={'amenities': 'amenities_len'})

bc_fitted_feature, bc_fitted_lambda = stats.boxcox(new_df.space_len+1)
new_df.space_len =  bc_fitted_feature
bc_fitted_feature, bc_fitted_lambda = stats.boxcox(new_df.description_len+1)
new_df.description_len =  bc_fitted_feature
bc_fitted_feature, bc_fitted_lambda = stats.boxcox(new_df.neighbourhood_overview_len+1)
new_df.neighbourhood_overview_len =  bc_fitted_feature
bc_fitted_feature, bc_fitted_lambda = stats.boxcox(new_df.house_len+1)
new_df.house_len =  bc_fitted_feature
bc_fitted_feature, bc_fitted_lambda = stats.boxcox(new_df.amenities_len+1)
new_df.amenities_len =  bc_fitted_feature

target_numerical_data = target_numerical_data.merge(new_df,on='listing_id')  #we merge it with the data frame of lengths we calculated priorly.
target_numerical_data #confirmation

with open('target_numerical_data10.pkl','wb') as path_name: #saved at this point of notebook
    pickle.dump(target_numerical_data, path_name)

with open('target_numerical_data11.pkl','wb') as path_name: #saved at this point of notebook
    pickle.dump(target_numerical_data, path_name)

with open('target_numerical_data10.pkl','rb') as path_name: #to reload the relevant data frames from this point of notebook
    target_numerical_data = pickle.load(path_name)

with open('target_numerical_data11.pkl','rb') as path_name:
    target_numerical_data = pickle.load(path_name)

"""##  Model Building"""

text_train_cleaned_copy = text_train_X_cleaned.copy()        #in case of errors below

text_train_cleaned_copy2 = text_train_X_cleaned.copy()

#text_train_cleaned = text_train_cleaned_copy2

text_test_cleaned_copy = text_test_X_cleaned.copy()        #in case of errors below

text_test_cleaned_copy2 = text_test_X_cleaned.copy()

#text_test_cleaned = text_test_cleaned_copy2

num_train_X_copy = num_train_X.copy()        #in case of errors below

num_train_X_copy2 = num_train_X.copy()

#num_train_X = num_train_X_copy2

num_test_X_copy = num_test_X.copy()        #in case of errors below

num_test_X_copy2 = num_test_X.copy()

#num_test_X = num_test_X_copy2

"""## Implementation of XGB"""

num_train_X.shape

num_train_y.shape

num_test_X.shape

num_test_y.shape

import xgboost as xgb
from sklearn.model_selection import RepeatedKFold, cross_val_score
splits = 6
cv_scoring = ('neg_root_mean_squared_error')

xgb_model = xgb.XGBRegressor(
    objective = 'reg:squarederror',
    learning_rate = 0.05,
    max_depth = 10,
    min_child_weight = 1,
    subsample = .1,
    colsample_bytree = .8,
    verbosity = 1
)

xgb_rkf = RepeatedKFold(n_splits = splits, n_repeats = 10, random_state = seed) #lastlastlast one -0.15020889813701313
xgb_scores = cross_val_score(xgb_model, num_train_X, num_train_y, cv = xgb_rkf, scoring = cv_scoring)
xgb_avg_rmse = np.mean(xgb_scores)
print('XGBoost: average root mean squarred error for {} folds is {}'.format(splits, xgb_avg_rmse)) #-36 before*2#-0.15044830118616423before

# fit XGBoost and make predictions
xgb_model.fit(num_train_X, num_train_y)

# plot feature importance for XGBoost #latitude 8233 #longitude6105
xgb.plot_importance(xgb_model)
plt.show()

"""#### LSTM"""

# Setting the input shapes of the respective variables
input_layer_name = Input(shape=(30, ))
input_layer_description = Input(shape=(197, ))
input_layer_neighbourhood_overview = Input(shape=(191, ))
input_layer_amenities = Input(shape=(200, ))
input_layer_transit = Input(shape=(146, ))

# Mapping the pre-trained vectors from Wiki2Vec to the variable "name"
embeds_name = Embedding(NUM_WORDS_NAME, output_dim=100,
                     embeddings_initializer=Constant(wiki_weights_name),
                     input_length=30, trainable=True)(input_layer_name)
name_variable_LSTM = LSTM(100, return_sequences=False)(embeds_name)
dense_layer_name = Dense(1, activation="relu")(name_variable_LSTM)

# Mapping the pre-trained vectors from Wiki2Vec to the variable "description"
embeds_description = Embedding(NUM_WORDS_DESCRIPTION, output_dim=100,
                     embeddings_initializer=Constant(wiki_weights_description),
                     input_length=197, trainable=True)(input_layer_description)
description_variable_LSTM = LSTM(100, return_sequences=False)(embeds_description)
dense_layer_description = Dense(1, activation="relu")(description_variable_LSTM)

# Mapping the pre-trained vectors from Wiki2Vec to the variable "neighbourhood_overview"
embeds_neighbourhood_overview = Embedding(NUM_WORDS_NEIGHBOURHOOD_OVERVIEW, output_dim=100,
                     embeddings_initializer=Constant(wiki_weights_neighbourhood_overview),
                     input_length=191, trainable=True)(input_layer_neighbourhood_overview)
neighbourhood_overview_variable_LSTM = LSTM(100, return_sequences=False)(embeds_neighbourhood_overview)
dense_layer_neighbourhood_overview = Dense(1, activation="relu")(neighbourhood_overview_variable_LSTM)

# Mapping the pre-trained vectors from Wiki2Vec to the variable "transit"
embeds_transit = Embedding(NUM_WORDS_TRANSIT, output_dim=100,
                     embeddings_initializer=Constant(wiki_weights_transit),
                     input_length=146, trainable=True)(input_layer_transit)
transit_variable_LSTM = LSTM(100, return_sequences=False)(embeds_transit)
dense_layer_transit = Dense(1, activation="relu")(transit_variable_LSTM)

# Mapping the pre-trained vectors from Wiki2Vec to the variable "amenities"
embeds_amenities = Embedding(NUM_WORDS_AMENITIES, output_dim=100,
                     embeddings_initializer=Constant(wiki_weights_amenities),
                     input_length=200, trainable=True)(input_layer_amenities)
amenities_variable_LSTM = LSTM(100, return_sequences=False)(embeds_amenities)
dense_layer_amenities = Dense(1, activation="relu")(amenities_variable_LSTM)



# We merge the dense layers with the concatenate function below
layer_of_output = concatenate([dense_layer_name, dense_layer_description, dense_layer_neighbourhood_overview, dense_layer_transit, dense_layer_amenities])

final_model = Model(inputs=[input_layer_name, input_layer_description, input_layer_neighbourhood_overview, input_layer_transit, input_layer_amenities], outputs = layer_of_output)
final_model.compile(loss = losses.LogCosh(), optimizer = "adam", metrics=['mae', 'mse'])

print(final_model.summary())
early_stopping = [EarlyStopping(monitor='val_loss', patience=3, verbose=1, min_delta=0.01, restore_best_weights=True)]

story = final_model.fit([X_tr_int_pad_name, X_tr_int_pad_description, X_tr_int_pad_neighbourhood_overview, X_tr_int_pad_transit, X_tr_int_pad_amenities],
                  text_train_y, epochs=15, verbose=1, batch_size=32, validation_split=0.2, callbacks=early_stopping)

predictions = pd.DataFrame(final_model.predict([X_ts_int_pad_name,X_ts_int_pad_description,X_ts_int_pad_neighbourhood_overview, X_ts_int_pad_transit, X_ts_int_pad_amenities])
                     , columns=["prediction_name","prediction_description","prediction_neighbourhood_overview", "prediction_transit", "prediction_amenities"], index=text_train_y.index)
predictions["actual"] = text_train_y

"""- The results being good is likely an outcome of us evaluating them on the training set prices because somehow the shape of the test set and the train set didn't match, although it matches when we check with the shape function."""

#Performance metrics for the variable name
print(f"Name MSE: {mean_squared_error(predictions.prediction_name, predictions.actual)}")
print(f"Name MAE: {mean_absolute_error(predictions.prediction_name, predictions.actual)}\n")

#Performance metrics for the variable description
print(f"Description MSE: {mean_squared_error(predictions.prediction_description, predictions.actual)}")
print(f"Description MAE: {mean_absolute_error(predictions.prediction_description, predictions.actual)}\n")

#Performance metrics for the variable neighbourhood_overview
print(f"Neighbourhood_overview MSE: {mean_squared_error(predictions.prediction_neighbourhood_overview, predictions.actual)}")
print(f"Neighbourhood_overview MAE: {mean_absolute_error(predictions.prediction_neighbourhood_overview, predictions.actual)}\n")

#Performance metrics for the variable transit
print(f"Transit_overview MSE: {mean_squared_error(predictions.prediction_transit, predictions.actual)}")
print(f"Transit_overview MAE: {mean_absolute_error(predictions.prediction_transit, predictions.actual)}\n")

#Performance metrics for the variable amenities
print(f"Amenities MSE: {mean_squared_error(predictions.prediction_amenities, predictions.actual)}")
print(f"Amenities MAE: {mean_absolute_error(predictions.prediction_amenities, predictions.actual)}\n")

"""#### Bidirectional LSTM"""

# Setting the input shapes of the respective variables
input_layer_name = Input(shape=(30, ))
input_layer_description = Input(shape=(197, ))
input_layer_neighbourhood_overview = Input(shape=(191, ))
input_layer_amenities = Input(shape=(200, ))
input_layer_transit = Input(shape=(146, ))

# # Mapping the pre-trained vectors from Wiki2Vec to the variable "name"
embeds_name = Embedding(NUM_WORDS_NAME, output_dim=100,
                     embeddings_initializer=Constant(wiki_weights_name),
                     input_length=30, trainable=True)(input_layer_name)
name_variable_bi_lstm = Bidirectional(LSTM(100, return_sequences=False),merge_mode="concat")(embeds_name)
dense_layer_name = Dense(1, activation="relu")(name_variable_bi_lstm)

# # Mapping the pre-trained vectors from Wiki2Vec to the variable "description"
embeds_description = Embedding(NUM_WORDS_DESCRIPTION, output_dim=100,
                     embeddings_initializer=Constant(wiki_weights_description),
                     input_length=197, trainable=True)(input_layer_description)
description_variable_bi_lstm = Bidirectional(LSTM(100, return_sequences=False),merge_mode="concat")(embeds_description)
dense_layer_description = Dense(1, activation="relu")(description_variable_bi_lstm)

# # Mapping the pre-trained vectors from Wiki2Vec to the variable "neighbourhood_overview"
embeds_neighbourhood_overview = Embedding(NUM_WORDS_NEIGHBOURHOOD_OVERVIEW, output_dim=100,
                     embeddings_initializer=Constant(wiki_weights_neighbourhood_overview),
                     input_length=191, trainable=True)(input_layer_neighbourhood_overview)
neighbourhood_overview_variable_bi_lstm = Bidirectional(LSTM(100, return_sequences=False),merge_mode="concat")(embeds_neighbourhood_overview)
dense_layer_neighbourhood_overview = Dense(1, activation="relu")(neighbourhood_overview_variable_bi_lstm)

# # Mapping the pre-trained vectors from Wiki2Vec to the variable "transit"
embeds_transit = Embedding(NUM_WORDS_TRANSIT, output_dim=100,
                     embeddings_initializer=Constant(wiki_weights_transit),
                     input_length=146, trainable=True)(input_layer_transit)
transit_variable_bi_lstm = Bidirectional(LSTM(100, return_sequences=False),merge_mode="concat")(embeds_transit)
dense_layer_transit = Dense(1, activation="relu")(transit_variable_bi_lstm)

# # Mapping the pre-trained vectors from Wiki2Vec to the variable "amenities"
embeds_amenities = Embedding(NUM_WORDS_AMENITIES, output_dim=100,
                     embeddings_initializer=Constant(wiki_weights_amenities),
                     input_length=200, trainable=True)(input_layer_amenities)
amenities_variable_bi_lstm = Bidirectional(LSTM(100, return_sequences=False),merge_mode="concat")(embeds_amenities)
dense_layer_amenities = Dense(1, activation="relu")(amenities_variable_bi_lstm)



# # We merge the dense layers with the concatenate function below
layer_of_output = concatenate([dense_layer_name, dense_layer_description, dense_layer_neighbourhood_overview, dense_layer_transit, dense_layer_amenities])

final_model = Model(inputs=[input_layer_name, input_layer_description, input_layer_neighbourhood_overview, input_layer_transit, input_layer_amenities], outputs = layer_of_output)
final_model.compile(loss = losses.LogCosh(), optimizer = "adam", metrics=['mae', 'mse'])

print(final_model.summary())
early_stopping = [EarlyStopping(monitor='val_loss', patience=3, verbose=1, min_delta=0.01, restore_best_weights=True)]

story = final_model.fit([X_tr_int_pad_name, X_tr_int_pad_description, X_tr_int_pad_neighbourhood_overview, X_tr_int_pad_transit, X_tr_int_pad_amenities],
                  text_train_y, epochs=15, verbose=1, batch_size=32, validation_split=0.2, callbacks=early_stopping)

predictions = pd.DataFrame(model.predict([X_ts_int_pad_name,X_ts_int_pad_description,X_ts_int_pad_neighbourhood_overview, X_ts_int_pad_transit, X_ts_int_pad_amenities])
                     , columns=["prediction_name","prediction_description","prediction_neighbourhood_overview", "prediction_transit", "prediction_amenities"], index=text_train_y.index)
predictions["actual"] = text_train_y

#Performance metrics for the variable name
print(f"Name MSE: {mean_squared_error(predictions.prediction_name, predictions.actual)}")
print(f"Name MAE: {mean_absolute_error(predictions.prediction_name, predictions.actual)}\n")

#Performance metrics for the variable description
print(f"Description MSE: {mean_squared_error(predictions.prediction_description, predictions.actual)}")
print(f"Description MAE: {mean_absolute_error(predictions.prediction_description, predictions.actual)}\n")

#Performance metrics for the variable neighbourhood_overview
print(f"Neighbourhood_overview MSE: {mean_squared_error(predictions.prediction_neighbourhood_overview, predictions.actual)}")
print(f"Neighbourhood_overview MAE: {mean_absolute_error(predictions.prediction_neighbourhood_overview, predictions.actual)}\n")

#Performance metrics for the variable transit
print(f"Transit_overview MSE: {mean_squared_error(predictions.prediction_transit, predictions.actual)}")
print(f"Transit_overview MAE: {mean_absolute_error(predictions.prediction_transit, predictions.actual)}\n")

#Performance metrics for the variable amenities
print(f"Amenities MSE: {mean_squared_error(predictions.prediction_amenities, predictions.actual)}")
print(f"Amenities MAE: {mean_absolute_error(predictions.prediction_amenities, predictions.actual)}\n")

"""**Interpretation:**
- We observed no clear signs if the bidirectional LSTM or the simple LSTM performed better thus cannot conclude whether one is superior than the other.

#### Full Model Architecture
"""

text_train_cleaned_copy = text_train_X_cleaned.copy()        #in case of errors below

text_train_cleaned_copy2 = text_train_X_cleaned.copy()

#text_train_cleaned = text_train_cleaned_copy2

text_test_cleaned_copy = text_test_X_cleaned.copy()        #in case of errors below

text_test_cleaned_copy2 = text_test_X_cleaned.copy()

#text_test_cleaned = text_test_cleaned_copy2

# Setting the input shapes of the respective variables
input_layer_name = Input(shape=(30, ))
input_layer_description = Input(shape=(197, ))
input_layer_neighbourhood_overview = Input(shape=(191, ))
input_layer_amenities = Input(shape=(200, ))
input_layer_transit = Input(shape=(146, ))

# Mapping the pre-trained vectors from Wiki2Vec to the variable "name"
embeds_name = Embedding(NUM_WORDS_NAME, output_dim=100,
                     embeddings_initializer=Constant(wiki_weights_name),
                     input_length=30, trainable=True)(input_layer_name)
name_variable_LSTM = LSTM(100, return_sequences=False)(embeds_name)
dense_layer_name = Dense(1, activation="relu")(name_variable_LSTM)

# Mapping the pre-trained vectors from Wiki2Vec to the variable "description"
embeds_description = Embedding(NUM_WORDS_DESCRIPTION, output_dim=100,
                     embeddings_initializer=Constant(wiki_weights_description),
                     input_length=197, trainable=True)(input_layer_description)
description_variable_LSTM = LSTM(100, return_sequences=False)(embeds_description)
dense_layer_description = Dense(1, activation="relu")(description_variable_LSTM)

# Mapping the pre-trained vectors from Wiki2Vec to the variable "neighbourhood_overview"
embeds_neighbourhood_overview = Embedding(NUM_WORDS_NEIGHBOURHOOD_OVERVIEW, output_dim=100,
                     embeddings_initializer=Constant(wiki_weights_neighbourhood_overview),
                     input_length=191, trainable=True)(input_layer_neighbourhood_overview)
neighbourhood_overview_variable_LSTM = LSTM(100, return_sequences=False)(embeds_neighbourhood_overview)
dense_layer_neighbourhood_overview = Dense(1, activation="relu")(neighbourhood_overview_variable_LSTM)

# Mapping the pre-trained vectors from Wiki2Vec to the variable "transit"
embeds_transit = Embedding(NUM_WORDS_TRANSIT, output_dim=100,
                     embeddings_initializer=Constant(wiki_weights_transit),
                     input_length=146, trainable=True)(input_layer_transit)
transit_variable_LSTM = LSTM(100, return_sequences=False)(embeds_transit)
dense_layer_transit = Dense(1, activation="relu")(transit_variable_LSTM)

# Mapping the pre-trained vectors from Wiki2Vec to the variable "amenities"
embeds_amenities = Embedding(NUM_WORDS_AMENITIES, output_dim=100,
                     embeddings_initializer=Constant(wiki_weights_amenities),
                     input_length=200, trainable=True)(input_layer_amenities)
amenities_variable_LSTM = LSTM(100, return_sequences=False)(embeds_amenities)
dense_layer_amenities = Dense(1, activation="relu")(amenities_variable_LSTM)



# We merge the dense layers with the concatenate function below
layer_of_output = concatenate([dense_layer_name, dense_layer_description, dense_layer_neighbourhood_overview, dense_layer_transit, dense_layer_amenities])

final_model = Model(inputs=[input_layer_name, input_layer_description, input_layer_neighbourhood_overview, input_layer_transit, input_layer_amenities], outputs = layer_of_output)
final_model.compile(loss = losses.LogCosh(), optimizer = "adam", metrics=['mae', 'mse'])

print(final_model.summary())
early_stopping = [EarlyStopping(monitor='val_loss', patience=3, verbose=1, min_delta=0.01, restore_best_weights=True)]

story = final_model.fit([X_tr_int_pad_name, X_tr_int_pad_description, X_tr_int_pad_neighbourhood_overview, X_tr_int_pad_transit, X_tr_int_pad_amenities],
                  num_train_y, epochs=15, verbose=1, batch_size=32, validation_split=0.2, callbacks=early_stopping)

#We evaluate the performance of the model above

model.evaluate([X_ts_int_pad_name,X_ts_int_pad_description,X_ts_int_pad_neighbourhood_overview, X_ts_int_pad_transit, X_ts_int_pad_amenities], num_train_y)

"""- The performance metrics suggests that the full architecture model performs slightly better than the previous ones."""

predictions = pd.DataFrame(model.predict([X_ts_int_pad_name,X_ts_int_pad_description,X_ts_int_pad_neighbourhood_overview, X_ts_int_pad_transit, X_ts_int_pad_amenities])
                     , columns=["prediction_name","prediction_description","prediction_neighbourhood_overview", "prediction_transit", "prediction_amenities"], index=text_train_X.index)
predictions["actual"] = text_test_y

"""#### Prediction of the Prices from the Target Data Set

- Let's check how our target data set look like at this point.
"""

target_numerical_data.describe()

"""- We first scaled all of the numeric variables and then normalized them. This might be the reason of values larger than 1 in the property_type and host_total_listings_count variables. Because of that reason, we will leave them as they are.

- We cleaned the text data while we were cleaning the text_train and text_test data sets but our text data still needs some further data pre-processing, which we will do below.
"""

target_text_cleaned.drop(['summary', 'space', 'house_rules'], axis=1, inplace=True)

"""- First we reorganize the whole training data, so that we can use as much data as we have for the prediction of the prices."""

text_data = text_train_X_cleaned.append(text_test_X_cleaned)
text_data

numeric_data = num_train_X.append(num_test_X)
numeric_data

y = num_train_y.append(num_test_y)
y

y_copy = y.copy()        #in case of errors below

y_copy2 = y.copy()

#y = y_copy2

numeric_data_copy = numeric_data.copy()        #in case of errors below

numeric_data_copy2 = numeric_data.copy()

#numeric_data = numeric_data_copy2 #za

text_data_copy = text_data.copy()        #in case of errors below

text_data_copy2 = text_data.copy()

#text_data = text_data_copy2

with open('numeric_data10.pkl','wb') as path_name: #saved at this point of notebook
    pickle.dump(numeric_data, path_name)

with open('numeric_data11.pkl','wb') as path_name: #saved at this point of notebook
    pickle.dump(numeric_data, path_name)

with open('text_data10.pkl','wb') as path_name: #saved at this point of notebook
    pickle.dump(text_data, path_name)

with open('text_data11.pkl','wb') as path_name: #saved at this point of notebook
    pickle.dump(text_data, path_name)

with open('y10.pkl','wb') as path_name: #saved at this point of notebook
    pickle.dump(y, path_name)

with open('y11.pkl','wb') as path_name: #saved at this point of notebook
    pickle.dump(y, path_name)

"""- Now, we are ready for the further preprocessing of the predicting the price process."""

tokenizer_name = Tokenizer(oov_token=1, filters='!"#$%&()*+,-.:;<=>?@[\\]^`{|}~\t\n', lower=False)
tokenizer_name.fit_on_texts(text_data.name)
NUM_WORDS_NAME = len(tokenizer_name.word_index) + 1

X_tr_int_name = tokenizer_name.texts_to_sequences(text_data.name)
max_text_length = max([len(article) for article in X_tr_int_name])
print('The longest text in the target data set, name variable has {} words.'.format(max_text_length))

# Upper bound of the variable length for padding
MAX_NAME_LENGTH = 30
X_tr_int_pad_name = pad_sequences(X_tr_int_name, MAX_NAME_LENGTH)

# Encode and pad the test data
X_ts_int_name = tokenizer_name.texts_to_sequences(text_data.name)
X_ts_int_pad_name = pad_sequences(X_ts_int_name, MAX_NAME_LENGTH)

wiki_weights_name, _ = get_embedding_matrix(tokenizer_name, wiki2vec, NUM_WORDS_NAME)



#
tokenizer_description = Tokenizer(oov_token=1, filters='!"#$%&()*+,-.:;<=>?@[\\]^`{|}~\t\n', lower=False)
tokenizer_description.fit_on_texts(text_data.description)
NUM_WORDS_DESCRIPTION = len(tokenizer_description.word_index) + 1

X_tr_int_description = tokenizer_description.texts_to_sequences(text_data.description)
max_text_length = max([len(article) for article in X_tr_int_description])
print('The longest text in the target data set, description variable has {} words.'.format(max_text_length))

# Upper bound of the variable length for padding
MAX_DESCRIPTION_LENGTH = 197
X_tr_int_pad_description = pad_sequences(X_tr_int_description, MAX_DESCRIPTION_LENGTH)

# Encode and pad the test data
X_ts_int_description = tokenizer_description.texts_to_sequences(text_data.description)
X_ts_int_pad_description = pad_sequences(X_ts_int_description, MAX_DESCRIPTION_LENGTH)

wiki_weights_description, _ = get_embedding_matrix(tokenizer_description, wiki2vec, NUM_WORDS_DESCRIPTION)



#
tokenizer_neighbourhood_overview = Tokenizer(oov_token=1, filters='!"#$%&()*+,-.:;<=>?@[\\]^`{|}~\t\n', lower=False)
tokenizer_neighbourhood_overview.fit_on_texts(text_data.neighbourhood_overview)
NUM_WORDS_NEIGHBOURHOOD_OVERVIEW = len(tokenizer_neighbourhood_overview.word_index) + 1

X_tr_int_neighbourhood_overview = tokenizer_neighbourhood_overview.texts_to_sequences(text_data.neighbourhood_overview)
max_text_length = max([len(article) for article in X_tr_int_neighbourhood_overview])
print('The longest text in the training data set, neighbourhood_overview variable has {} words.'.format(max_text_length))

# Upper bound of the variable length for padding
MAX_NEIGHBOURHOOD_OVERVIEW_LENGTH = 191
X_tr_int_pad_neighbourhood_overview = pad_sequences(X_tr_int_neighbourhood_overview, MAX_NEIGHBOURHOOD_OVERVIEW_LENGTH)

# Encode and pad the test data
X_ts_int_neighbourhood_overview = tokenizer_neighbourhood_overview.texts_to_sequences(text_data.neighbourhood_overview)
X_ts_int_pad_neighbourhood_overview = pad_sequences(X_ts_int_neighbourhood_overview, MAX_NEIGHBOURHOOD_OVERVIEW_LENGTH)

wiki_weights_neighbourhood_overview, _ = get_embedding_matrix(tokenizer_neighbourhood_overview, wiki2vec, NUM_WORDS_NEIGHBOURHOOD_OVERVIEW)



#
tokenizer_transit = Tokenizer(oov_token=1, filters='!"#$%&()*+,-.:;<=>?@[\\]^`{|}~\t\n', lower=False)
tokenizer_transit.fit_on_texts(text_data.transit)
NUM_WORDS_TRANSIT = len(tokenizer_transit.word_index) + 1

X_tr_int_transit = tokenizer_transit.texts_to_sequences(text_data.transit)
max_text_length = max([len(article) for article in X_tr_int_transit])
print('The longest text in the training data set, transit variable has {} words.'.format(max_text_length))

# Upper bound of the variable length for padding
MAX_TRANSIT_LENGTH = 146
X_tr_int_pad_transit = pad_sequences(X_tr_int_transit, MAX_TRANSIT_LENGTH)

# Encode and pad the test data
X_ts_int_transit = tokenizer_transit.texts_to_sequences(text_data.transit)
X_ts_int_pad_transit = pad_sequences(X_ts_int_transit, MAX_TRANSIT_LENGTH)

wiki_weights_transit, _ = get_embedding_matrix(tokenizer_transit, wiki2vec, NUM_WORDS_TRANSIT)



#
tokenizer_amenities = Tokenizer(oov_token=1, filters='!"#$%&()*+,-.:;<=>?@[\\]^`{|}~\t\n', lower=False)
tokenizer_amenities.fit_on_texts(text_data.amenities)
NUM_WORDS_AMENITIES = len(tokenizer_amenities.word_index) + 1

X_tr_int_amenities = tokenizer_amenities.texts_to_sequences(text_data.amenities)
max_text_length = max([len(article) for article in X_tr_int_amenities])
print('The longest text in the training data set, amenities variable has {} words.'.format(max_text_length))

# Upper bound of the variable length for padding
MAX_AMENITIES_LENGTH = 200
X_tr_int_pad_amenities = pad_sequences(X_tr_int_amenities, MAX_AMENITIES_LENGTH)

# Encode and pad the test data
X_ts_int_amenities = tokenizer_amenities.texts_to_sequences(text_data.amenities)
X_ts_int_pad_amenities = pad_sequences(X_ts_int_amenities, MAX_AMENITIES_LENGTH)

wiki_weights_amenities, _ = get_embedding_matrix(tokenizer_amenities, wiki2vec, NUM_WORDS_AMENITIES)

"""- Eventually, we decided to predict the prices using a deep neural network with LSTM."""

text_data

data_agg = numeric_data.merge(text_data,on='listing_id') #all the known data merged
data_agg

numeric_data.shape

# Setting the input shapes of the respective variables
input_layer_name = Input(shape=(30, ))
input_layer_description = Input(shape=(197, ))
input_layer_neighbourhood_overview = Input(shape=(191, ))
input_layer_amenities = Input(shape=(200, ))
input_layer_transit = Input(shape=(146, ))
input_nontext = Input(shape=(17, ))

# Mapping the pre-trained vectors from Wiki2Vec to the variable "name"
embeds_name = Embedding(NUM_WORDS_NAME, output_dim=100,
                     embeddings_initializer=Constant(wiki_weights_name),
                     input_length=30, trainable=True)(input_layer_name)
name_variable_LSTM = LSTM(100, return_sequences=False)(embeds_name)
dense_layer_name = Dense(1, activation="relu")(name_variable_LSTM)

# Mapping the pre-trained vectors from Wiki2Vec to the variable "description"
embeds_description = Embedding(NUM_WORDS_DESCRIPTION, output_dim=100,
                     embeddings_initializer=Constant(wiki_weights_description),
                     input_length=197, trainable=True)(input_layer_description)
description_variable_LSTM = LSTM(100, return_sequences=False)(embeds_description)
dense_layer_description = Dense(1, activation="relu")(description_variable_LSTM)

# Mapping the pre-trained vectors from Wiki2Vec to the variable "neighbourhood_overview"
embeds_neighbourhood_overview = Embedding(NUM_WORDS_NEIGHBOURHOOD_OVERVIEW, output_dim=100,
                     embeddings_initializer=Constant(wiki_weights_neighbourhood_overview),
                     input_length=191, trainable=True)(input_layer_neighbourhood_overview)
neighbourhood_overview_variable_LSTM = LSTM(100, return_sequences=False)(embeds_neighbourhood_overview)
dense_layer_neighbourhood_overview = Dense(1, activation="relu")(neighbourhood_overview_variable_LSTM)

# Mapping the pre-trained vectors from Wiki2Vec to the variable "transit"
embeds_transit = Embedding(NUM_WORDS_TRANSIT, output_dim=100,
                     embeddings_initializer=Constant(wiki_weights_transit),
                     input_length=146, trainable=True)(input_layer_transit)
transit_variable_LSTM = LSTM(100, return_sequences=False)(embeds_transit)
dense_layer_transit = Dense(1, activation="relu")(transit_variable_LSTM)

# Mapping the pre-trained vectors from Wiki2Vec to the variable "amenities"
embeds_amenities = Embedding(NUM_WORDS_AMENITIES, output_dim=100,
                     embeddings_initializer=Constant(wiki_weights_amenities),
                     input_length=200, trainable=True)(input_layer_amenities)
amenities_variable_LSTM = LSTM(100, return_sequences=False)(embeds_amenities)
dense_layer_amenities = Dense(1, activation="relu")(amenities_variable_LSTM)



# We merge the dense layers with the concatenate function below
concat = concatenate([dense_layer_name, dense_layer_description, dense_layer_neighbourhood_overview, dense_layer_transit, dense_layer_amenities, input_nontext])
dense_full = Dense(1024, activation="relu")(concat)
dense_full = Dense(512, activation="relu")(dense_full)
dense_full = Dense(256, activation="relu")(dense_full)
layer_of_output = Dense(1, activation="linear")(dense_full)

final_model = Model(inputs=[input_layer_name, input_layer_description, input_layer_neighbourhood_overview, input_layer_transit, input_layer_amenities, input_nontext], outputs = layer_of_output)
final_model.compile(loss = losses.LogCosh(), optimizer = "adam", metrics=['mae', 'mse'])

print(final_model.summary())
early_stopping = [EarlyStopping(monitor='val_loss', patience=3, verbose=1, min_delta=0.01, restore_best_weights=True)]

story = final_model.fit([X_tr_int_pad_name, X_tr_int_pad_description, X_tr_int_pad_neighbourhood_overview, X_tr_int_pad_transit, X_tr_int_pad_amenities, numeric_data],
                  y, epochs=1, verbose=1, batch_size=32, validation_split=0.2, callbacks=early_stopping)

"""- In a version of this notebook we trained the model above with 15 epochs but later realized that one of the data sets that we used didn't match the one that we would use for predicting the prices. So, we had to run only 1 epoch to get a quick result."""

#For Name
target_text_cleaned_int_name = tokenizer_name.texts_to_sequences(target_text_cleaned.name)
target_text_cleaned_int_pad_name = pad_sequences(target_text_cleaned_int_name, MAX_NAME_LENGTH)

#For Description
target_text_cleaned_int_description = tokenizer_description.texts_to_sequences(target_text_cleaned.description)
target_text_cleaned_int_pad_description = pad_sequences(target_text_cleaned_int_description, MAX_DESCRIPTION_LENGTH)

#For Neighbourhood_overview
target_text_cleaned_int_neighbourhood_overview = tokenizer_neighbourhood_overview.texts_to_sequences(target_text_cleaned.neighbourhood_overview)
target_text_cleaned_int_pad_neighbourhood_overview = pad_sequences(target_text_cleaned_int_neighbourhood_overview, MAX_NEIGHBOURHOOD_OVERVIEW_LENGTH)

#For Transit
target_text_cleaned_int_transit = tokenizer_transit.texts_to_sequences(target_text_cleaned.transit)
target_text_cleaned_int_pad_transit = pad_sequences(target_text_cleaned_int_transit, MAX_TRANSIT_LENGTH)

#For Amenities
target_text_cleaned_int_amenities = tokenizer_amenities.texts_to_sequences(target_text_cleaned.amenities)
target_text_cleaned_int_pad_amenities = pad_sequences(target_text_cleaned_int_amenities, MAX_AMENITIES_LENGTH)

numeric_data

target_numerical_data

#target_numerical_data #za
target_numerical_data.drop(['space_len_y', 'amenities_len_y', 'neighbourhood_overview_len_y', 'description_len_y', 'house_len_y'], axis=1, inplace=True)

target_numerical_data_copy =target_numerical_data.copy()

target_numerical_data_copy2 =target_numerical_data.copy()

numeric_data

target_numerical_data

target_numerical_data = target_numerical_data.drop_duplicates()
target_numerical_data

target_text_cleaned_int_pad_name_copy=target_text_cleaned_int_pad_name.copy()
target_text_cleaned_int_pad_name_copy=target_text_cleaned_int_pad_description.copy()
target_text_cleaned_int_pad_neighbourhood_overview_copy=target_text_cleaned_int_pad_neighbourhood_overview.copy()
target_text_cleaned_int_pad_transit_copy=target_text_cleaned_int_pad_transit.copy()
target_text_cleaned_int_pad_amenities_copy=target_text_cleaned_int_pad_amenities.copy()

target_text_cleaned_int_pad_amenities

with open('target_text_cleaned_int_pad_name1.pkl','wb') as path_name: #this pickle file was created at this point of notebook when rerunning
    pickle.dump(target_text_cleaned_int_pad_name, path_name)

with open('target_text_cleaned_int_pad_description1.pkl','wb') as path_name: #this pickle file was created at this point of notebook when rerunning
    pickle.dump(target_text_cleaned_int_pad_description, path_name)

with open('target_text_cleaned_int_pad_neighbourhood_overview1.pkl','wb') as path_name: #this pickle file was created at this point of notebook when rerunning
    pickle.dump(target_text_cleaned_int_pad_neighbourhood_overview, path_name)

with open('target_text_cleaned_int_pad_transit1.pkl','wb') as path_name: #this pickle file was created at this point of notebook when rerunning
    pickle.dump(target_text_cleaned_int_pad_transit, path_name)

with open('target_text_cleaned_int_pad_amenities1.pkl','wb') as path_name: #this pickle file was created at this point of notebook when rerunning
    pickle.dump(target_text_cleaned_int_pad_amenities, path_name)

#target_text_cleaned_int_pad_name
n = 3  # Remove last 3 rows of array.
target_text_cleaned_int_pad_name = target_text_cleaned_int_pad_name[:-n, :]
target_text_cleaned_int_pad_description = target_text_cleaned_int_pad_description[:-n, :]
target_text_cleaned_int_pad_neighbourhood_overview = target_text_cleaned_int_pad_neighbourhood_overview[:-n, :]
target_text_cleaned_int_pad_transit = target_text_cleaned_int_pad_transit[:-n, :]
target_text_cleaned_int_pad_amenities = target_text_cleaned_int_pad_amenities[:-n, :]

target_text_cleaned_int_pad_name.shape

trial_df = target_numerical_data.transpose()
trial_df

trial_df.shape

trial_df = target_numerical_data.rename(columns={'space_len_x': 'space_len'}) #name fixation
trial_df = trial_df.rename(columns={'amenities_len_x': 'amenities_len'})
trial_df = trial_df.rename(columns={'neighbourhood_overview_len_x': 'neighbourhood_overview_len'})
trial_df = trial_df.rename(columns={'description_len_x': 'description_len'})
trial_df = trial_df.rename(columns={'house_len_x': 'house_len'})

trial_df

numeric_data

target_numerical_data

trial_df=  trial_df.to_numpy()
trial_df

target_text_cleaned_int_pad_name.shape

target_text_cleaned_int_pad_description.shape

target_text_cleaned_int_pad_neighbourhood_overview.shape

target_text_cleaned_int_pad_transit.shape

target_text_cleaned_int_pad_amenities.shape

trial_df.shape

numeric_data.shape

trial_df=  target_numerical_data.to_numpy()
trial_df

final_predictions = pd.DataFrame(final_model.predict([target_text_cleaned_int_pad_name, target_text_cleaned_int_pad_description, target_text_cleaned_int_pad_neighbourhood_overview, target_text_cleaned_int_pad_transit, target_text_cleaned_int_pad_amenities, target_numerical_data]), columns=["price"], index=target_numerical_data.index)
final_predictions

final_predictions2 = final_predictions*104.308754 #the mean price calculated after truncation
final_predictions2

final_predictions2.to_csv("final_price_predictions.csv")

"""#### Summary
- In the end the course and the task in general was quite useful and informative.
- We could write our codes more efficiently, organize them in a better way but we just spend way too much time with the data preparation and trying to decide on the optimal way of choosing feature and deciding on everything at once without starting coding and in the end little time was left for the coding part.

- We could have used the reviews data set. We tried to clean it and it took more than 9 hours, and we basically computationally couldn't afford it (The need for LEQR was discovered late, maybe it could be an idea to make it kind of compulsary to use it next year as also lots of my peer students have suffered from the same problem)

- We analyzed the text data manually, which was not a good idea. We also tried to think of things that would indicate high prices although this should be the algorithms task, which caused us to lose some time.

- We could have made use of the image data.

- We could have analyzed the text better, add some visualization techniques, do sentiment analysis.

- We tried GRU, Random Forest, Linear Regression but we didn't report it here however, we could have tried more combinations of bringing the text and the numerical data together. (We guess one has to try and run a model to see if an idea would result in a good accuracy)

- We could have used models that make use of transfer learning, attention mechanisms such as BERT or GPT3, by adding an extra layer on top of them to adjust them to our problem rather then a classification problem.




- Overall, we still did our best, course was enjoyed, quite a lot of new information was learned.
"""